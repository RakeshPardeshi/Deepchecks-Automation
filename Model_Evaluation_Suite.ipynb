{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef5a7844-2797-4c39-a4f8-690123c98685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import deepchecks\n",
    "import deepchecks.tabular.checks as checks\n",
    "import ipywidgets as widgets\n",
    "from deepchecks.tabular import Suite, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_and_save_model(train_dataset, filepath='model.pkl'):\n",
    "    \"\"\"Train a sample RandomForest model and save it to a pickle file.\"\"\"\n",
    "    train_df = train_dataset.dataframe\n",
    "    X_train = train_df.drop(columns=['label'])\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    \n",
    "    print(f\"Model saved to {filepath}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7b4987-6139-4e8f-bd81-2868d2bde781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_pickle(filepath='model.pkl'):\n",
    "    \"\"\"Load a trained model from a pickle file.\"\"\"\n",
    "    with open(filepath, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc5e588a-0dde-4974-89ee-f3ba8fec2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_sample_model(train_dataset):\n",
    "#    \"\"\"Train a sample RandomForest model.\"\"\"\n",
    "#    train_df = train_dataset.data\n",
    "#    X_train = train_df.drop(columns=['label'])\n",
    "#    y_train = train_df['label']\n",
    "#    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "#    model.fit(X_train, y_train)\n",
    "#    return model\n",
    "def generate_sample_data():\n",
    "    \"\"\"Generate sample train and test datasets.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    data = pd.DataFrame({\n",
    "        'feature1': np.random.normal(0, 1, 2000),\n",
    "        'feature2': np.random.normal(5, 2, 2000),\n",
    "        'feature3': np.random.randint(0, 2, 2000),\n",
    "        'label': np.random.randint(0, 2, 2000)\n",
    "    })\n",
    "    \n",
    "    train_data, test_data = train_test_split(data, test_size=0.5, random_state=42)\n",
    "    train_dataset = Dataset(train_data, label='label')\n",
    "    test_dataset = Dataset(test_data, label='label')\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_sample_model(train_dataset, test_dataset, filepath='model.pkl'):\n",
    "    \"\"\"Train a sample RandomForest model and add predicted probabilities and labels.\"\"\"\n",
    "    train_df = train_dataset.data\n",
    "    test_df = test_dataset.data\n",
    "\n",
    "    X_train = train_df.drop(columns=['label'])\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    X_test = test_df.drop(columns=['label'])\n",
    "    \n",
    "    # Train the model\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    \n",
    "    print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    # Add predicted probabilities and labels\n",
    "    train_df[\"predicted_probability\"] = model.predict_proba(X_train)[:, 1]  # Assuming binary classification\n",
    "    train_df[\"predicted_label\"] = model.predict(X_train)\n",
    "    \n",
    "    test_df[\"predicted_probability\"] = model.predict_proba(X_test)[:, 1]\n",
    "    test_df[\"predicted_label\"] = model.predict(X_test)\n",
    "\n",
    "    return model, train_df, test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d59cffd-a603-445a-af79-92b15cc7d8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 1 categorical features were inferred.: feature3\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 1 categorical features were inferred.: feature3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model.pkl\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "train_dataset, test_dataset = generate_sample_data()\n",
    "model, train_dataset, test_dataset = train_sample_model(train_dataset, test_dataset)\n",
    "\n",
    "train_dataset.to_csv(r\"train.csv\", index = False)\n",
    "test_dataset.to_csv(r\"test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d172db-8b45-465a-8a24-5c068b41db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 2 categorical features were inferred.: feature3, predicted_label\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_probability</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.384065</td>\n",
       "      <td>8.499168</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>-0.219101</td>\n",
       "      <td>4.899591</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>-1.525525</td>\n",
       "      <td>5.170419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.640843</td>\n",
       "      <td>2.516479</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.562969</td>\n",
       "      <td>3.775527</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>0.321357</td>\n",
       "      <td>4.821532</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>0.081829</td>\n",
       "      <td>2.585957</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.202923</td>\n",
       "      <td>3.937571</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>0.673181</td>\n",
       "      <td>2.656691</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>0.302635</td>\n",
       "      <td>1.181288</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature1  feature2  feature3  label  predicted_probability  \\\n",
       "440   0.384065  8.499168         0      0                   0.42   \n",
       "573  -0.219101  4.899591         1      0                   0.04   \n",
       "946  -1.525525  5.170419         0      0                   0.32   \n",
       "997   0.640843  2.516479         1      0                   0.22   \n",
       "503   0.562969  3.775527         0      0                   0.28   \n",
       "...        ...       ...       ...    ...                    ...   \n",
       "1130  0.321357  4.821532         1      0                   0.12   \n",
       "1294  0.081829  2.585957         0      0                   0.12   \n",
       "860   0.202923  3.937571         0      1                   0.84   \n",
       "1459  0.673181  2.656691         1      1                   0.70   \n",
       "1126  0.302635  1.181288         1      1                   0.78   \n",
       "\n",
       "      predicted_label  \n",
       "440                 0  \n",
       "573                 0  \n",
       "946                 0  \n",
       "997                 0  \n",
       "503                 0  \n",
       "...               ...  \n",
       "1130                0  \n",
       "1294                0  \n",
       "860                 1  \n",
       "1459                1  \n",
       "1126                1  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset\n",
    "tmp=Dataset(train_dataset, label = label)\n",
    "tmp.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a434be-3066-4bbb-8a53-181df7a26b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature1' 'feature2' 'feature3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['feature1', 'feature2', 'feature3', 'label']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.feature_names_in_)\n",
    "#list(model.feature_names_in_).append(label)\n",
    "feat_list = list(model.feature_names_in_)\n",
    "feat_list.append(label)\n",
    "feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee259dcd-8208-4823-9713-c91df9830e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import deepchecks\n",
    "import deepchecks.tabular.checks as checks\n",
    "import ipywidgets as widgets\n",
    "from deepchecks.tabular import Suite, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"Get user-defined parameter values for checks that require input.\"\"\"\n",
    "    params = {\n",
    "        'drift_threshold': 0.1,\n",
    "        'correlation_threshold': 0.9,\n",
    "        'max_missing_ratio': 0.05,\n",
    "        'min_accuracy': 0.7,\n",
    "        'max_overfit_ratio': 1.5\n",
    "    }\n",
    "    return params\n",
    "\n",
    "def get_model_evaluation_checks(params):\n",
    "    \"\"\"Retrieve and organize model evaluation checks into an OrderedDict with conditions.\"\"\"\n",
    "    check_dict_default = OrderedDict({\n",
    "        0: checks.TrainTestPerformance().add_condition_test_performance_greater_than(min_score = 0.20),\n",
    "        1: checks.RocReport().add_condition_auc_greater_than(min_auc = 0.7),\n",
    "        2: checks.SimpleModelComparison(),\n",
    "        4: checks.CalibrationScore()\n",
    "    })\n",
    "\n",
    "    check_dict_default = OrderedDict({\n",
    "    0: checks.TrainTestPerformance().add_condition_test_performance_greater_than(min_score = 0.20),\n",
    "    1: checks.RocReport().add_condition_auc_greater_than(min_auc = 0.7),\n",
    "    2: checks.SimpleModelComparison(),\n",
    "    4: checks.CalibrationScore()\n",
    "    })\n",
    "    \n",
    "    return check_dict_default\n",
    "\n",
    "def create_model_evaluation_suite():\n",
    "    \"\"\"Create a Model Evaluation suite with configured conditions.\"\"\"\n",
    "    params = get_user_input()\n",
    "    check_dict = get_model_evaluation_checks(params)\n",
    "    suite = Suite(\"Model Evaluation Suite\", *check_dict.values())\n",
    "    return suite\n",
    "\n",
    "def evaluate_model(model, train_dataset, test_dataset):\n",
    "\n",
    "    print(\"Creating datasets...\")\n",
    "    #y_pred_train = train_dataset[prediction_label_column]\n",
    "    #y_pred_test = test_dataset[prediction_label_column]\n",
    "\n",
    "    y_pred_train = train_dataset.pop(prediction_label_column)\n",
    "    y_pred_test = test_dataset.pop(prediction_label_column)\n",
    "    print(y_pred_train)\n",
    "\n",
    "    train_dataset = Dataset(train_dataset, label = label)\n",
    "    test_dataset = Dataset(test_dataset, label = label)\n",
    "    \n",
    "    \n",
    "    \"\"\"Run model evaluation checks using deepchecks.\"\"\"\n",
    "    if prediction_label_column is not None:\n",
    "\n",
    "        #train_dataset = Dataset(train_dataset, label = label)\n",
    "        #test_dataset = Dataset(test_dataset, label = label)\n",
    "        \n",
    "        suite1 = create_model_evaluation_suite()\n",
    "        result1 = suite1.run(train_dataset = train_dataset, \n",
    "                            test_dataset = test_dataset, \n",
    "                            y_pred_train = y_pred_train, \n",
    "                            y_pred_test = y_pred_test)\n",
    "\n",
    "        result1.show()\n",
    "\n",
    "    \n",
    "    if model is not None:\n",
    "        feat_list = model.feature_names_in_\n",
    "        feat_list = list(model.feature_names_in_)\n",
    "        feat_list.append(label)\n",
    "        print(feat_list)\n",
    "        display(train_dataset)\n",
    "        train_dataset = Dataset(train_dataset.data[feat_list], label=label)\n",
    "        test_dataset = Dataset(test_dataset.data[feat_list], label=label)\n",
    "        #display(train_dataset)\n",
    "        suite2 = create_model_evaluation_suite()\n",
    "        result2 = suite2.run(train_dataset = train_dataset, test_dataset = test_dataset, model = model)\n",
    "        result2.show()\n",
    "\n",
    "    return result1, result2\n",
    "\n",
    "# Example Usage:\n",
    "# suite = create_train_test_suite()\n",
    "# train_dataset, test_dataset = generate_sample_data()\n",
    "# suite.run(train_dataset, test_dataset)\n",
    "\n",
    "# Running Model Evaluation:\n",
    "# model = train_sample_model(train_dataset)\n",
    "# evaluate_model(model, train_dataset, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4466dca4-6768-4e2a-a0c4-648ebffe051e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 1 categorical features were inferred.: feature3\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 1 categorical features were inferred.: feature3\n",
      "deepchecks - WARNING - train and test datasets have common index - adding \"train\"/\"test\" prefixes. To avoid that provide datasets with no common indexes or pass the model object instead of the predictions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded train.csv with shape: (1000, 6)\n",
      "Loaded test.csv with shape: (1000, 6)\n",
      "Creating datasets...\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "995    0\n",
      "996    0\n",
      "997    1\n",
      "998    1\n",
      "999    1\n",
      "Name: predicted_label, Length: 1000, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - Could not find model's classes, using the observed classes. In order to make sure the classes used by the model are inferred correctly, please use the model_classes argument\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a2bd0f89ba4d36969e3ea41f31de85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_4XC0HJ3TMN8HMPIWKAFPTCBH5\">Model Evaluation S…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature1', 'feature2', 'feature3', 'label']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h4><b>Dataset Description</b></h4><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"min-width: 15px;\"></th>\n",
       "      <th style=\"min-width: 15px;\">Column</th>\n",
       "      <th style=\"min-width: 15px;\">DType</th>\n",
       "      <th style=\"min-width: 15px;\">Kind</th>\n",
       "      <th style=\"min-width: 15px;\">Additional Info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>integer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feature1</td>\n",
       "      <td>floating</td>\n",
       "      <td>Numerical Feature</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature2</td>\n",
       "      <td>floating</td>\n",
       "      <td>Numerical Feature</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feature3</td>\n",
       "      <td>integer</td>\n",
       "      <td>Categorical Feature</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>predicted_probability</td>\n",
       "      <td>floating</td>\n",
       "      <td>Numerical Feature</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><h4><b>Dataset Content</b></h4><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"min-width: 15px;\"></th>\n",
       "      <th style=\"min-width: 15px;\">label</th>\n",
       "      <th style=\"min-width: 15px;\">feature1</th>\n",
       "      <th style=\"min-width: 15px;\">feature2</th>\n",
       "      <th style=\"min-width: 15px;\">feature3</th>\n",
       "      <th style=\"min-width: 15px;\">predicted_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.384065</td>\n",
       "      <td>8.499168</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.219101</td>\n",
       "      <td>4.899591</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.525525</td>\n",
       "      <td>5.170419</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.640843</td>\n",
       "      <td>2.516479</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.562969</td>\n",
       "      <td>3.775527</td>\n",
       "      <td>0</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>0.321357</td>\n",
       "      <td>4.821532</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0.081829</td>\n",
       "      <td>2.585957</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "      <td>0.202923</td>\n",
       "      <td>3.937571</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1</td>\n",
       "      <td>0.673181</td>\n",
       "      <td>2.656691</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1</td>\n",
       "      <td>0.302635</td>\n",
       "      <td>1.181288</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 1 categorical features were inferred.: feature3\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 1 categorical features were inferred.: feature3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0739edf73ca54da9b57193ed10585be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_Z4MME56EVRDRSSW3BQDUQ3U8H\">Model Evaluation S…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Model Evaluation Suite, Model Evaluation Suite)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_csv(file_path):\n",
    "    \"\"\"Load CSV file into a Pandas DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Uploaded file is empty.\")\n",
    "        print(f\"Loaded {file_path} with shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "train_path = \"train.csv\"\n",
    "test_path = \"test.csv\"\n",
    "label = \"label\"\n",
    "model_file = \"model.pkl\"\n",
    "prediction_label_column = \"predicted_label\"\n",
    "probability_column = \"predicted_probability\"\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Loading data...\")\n",
    "train_df = load_csv(train_path)\n",
    "test_df = load_csv(test_path)    \n",
    "\n",
    "evaluate_model(model, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624228c3-abb6-42e3-81db-03a3139cdad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f6c90-3d77-4c12-83b6-f93d75c385de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60878d0a-fa11-45b7-89d3-3a8ec61efc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55736e-46b0-4ae8-a78d-cc593ce276e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da71633c-d21f-4d05-a113-25adcd9b037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import deepchecks\n",
    "import deepchecks.tabular.checks as checks\n",
    "import ipywidgets as widgets\n",
    "from deepchecks.tabular import Suite, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"Get user-defined parameter values for checks that require input.\"\"\"\n",
    "    params = {\n",
    "        'drift_threshold': 0.1,\n",
    "        'correlation_threshold': 0.9,\n",
    "        'max_missing_ratio': 0.05,\n",
    "        'min_accuracy': 0.7,\n",
    "        'max_overfit_ratio': 1.5\n",
    "    }\n",
    "    return params\n",
    "\n",
    "#def get_train_test_checks(params):\n",
    "#    \"\"\"Retrieve and organize train-test validation checks into an OrderedDict with conditions.\"\"\"\n",
    "#    check_dict = OrderedDict({\n",
    "#        0: checks.TrainTestFeatureDrift().add_condition_drift_score_less_than(params['drift_threshold']),\n",
    "#        1: checks.TrainTestLabelDrift().add_condition_drift_score_less_than(params['drift_threshold']),\n",
    "#        2: checks.TrainTestFeatureCorrelation().add_condition_correlation_less_than(params['correlation_threshold']),\n",
    "#        3: checks.TrainTestLabelCorrelationChange(),\n",
    "#        4: checks.TrainTestPredictionDrift().add_condition_drift_score_less_than(params['drift_threshold']),\n",
    "#        5: checks.TrainTestSamplesMix(),\n",
    "#        6: checks.TrainTestMissingValuesComparison().add_condition_max_missing_fraction_less_than(params['max_missing_ratio']),\n",
    "#        7: checks.TrainTestDuplicateSamples(),\n",
    "#        8: checks.TrainTestCategoryMismatch()\n",
    "#    })\n",
    "#    return check_dict\n",
    "\n",
    "#def create_train_test_suite():\n",
    "#    \"\"\"Create a Train-Test Evaluation suite with configured conditions.\"\"\"\n",
    "#    params = get_user_input()\n",
    "#    check_dict = get_train_test_checks(params)\n",
    "#    suite = Suite(\"Train-Test Evaluation Suite\", *check_dict.values())\n",
    "#    return suite\n",
    "\n",
    "def get_model_evaluation_checks(params):\n",
    "    \"\"\"Retrieve and organize model evaluation checks into an OrderedDict with conditions.\"\"\"\n",
    "    check_dict = OrderedDict({\n",
    "        #0: checks.ModelPerformanceReport().add_condition_accuracy_greater_than(params['min_accuracy']),\n",
    "        0: checks.TrainTestPerformance().add_condition_test_performance_greater_than(min_score = 0.20),\n",
    "        #1: checks.OverfitDetector().add_condition_overfit_ratio_less_than(params['max_overfit_ratio']),\n",
    "        1: checks.RocReport().add_condition_auc_greater_than(min_auc = 0.7),\n",
    "        2: checks.SimpleModelComparison(),\n",
    "        #3: checks.RobustnessReport(),\n",
    "        4: checks.CalibrationScore(),\n",
    "        #5: FeatureImportanceCheck()  # Custom Feature Importance Check\n",
    "\n",
    "    })\n",
    "    return check_dict\n",
    "\n",
    "def create_model_evaluation_suite():\n",
    "    \"\"\"Create a Model Evaluation suite with configured conditions.\"\"\"\n",
    "    params = get_user_input()\n",
    "    check_dict = get_model_evaluation_checks(params)\n",
    "    suite = Suite(\"Model Evaluation Suite\", *check_dict.values())\n",
    "    return suite\n",
    "\n",
    "def generate_sample_data():\n",
    "    \"\"\"Generate sample train and test datasets.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    data = pd.DataFrame({\n",
    "        'feature1': np.random.normal(0, 1, 2000),\n",
    "        'feature2': np.random.normal(5, 2, 2000),\n",
    "        'feature3': np.random.randint(0, 2, 2000),\n",
    "        'label': np.random.randint(0, 2, 2000)\n",
    "    })\n",
    "    \n",
    "    train_data, test_data = train_test_split(data, test_size=0.5, random_state=42)\n",
    "    train_dataset = Dataset(train_data, label='label')\n",
    "    test_dataset = Dataset(test_data, label='label')\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def train_sample_model(train_dataset):\n",
    "    \"\"\"Train a sample RandomForest model.\"\"\"\n",
    "    train_df = train_dataset.data\n",
    "    X_train = train_df.drop(columns=['label'])\n",
    "    y_train = train_df['label']\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, train_dataset, test_dataset):\n",
    "    \"\"\"Run model evaluation checks using deepchecks.\"\"\"\n",
    "    suite = create_model_evaluation_suite()\n",
    "    result = suite.run(train_dataset, test_dataset, model)\n",
    "    #result.show()\n",
    "    return result\n",
    "\n",
    "# Example Usage:\n",
    "# suite = create_train_test_suite()\n",
    "# train_dataset, test_dataset = generate_sample_data()\n",
    "# suite.run(train_dataset, test_dataset)\n",
    "\n",
    "# Running Model Evaluation:\n",
    "# model = train_sample_model(train_dataset)\n",
    "# evaluate_model(model, train_dataset, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df38d8b-2e83-4413-96bb-b46d7d5106ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset, test_dataset = generate_sample_data()\n",
    "#train_dataset.data.to_csv(r\"C:\\Users\\DELL\\Downloads\\train_dataset.csv\", index = False)\n",
    "#test_dataset.data.to_csv(r\"C:\\Users\\DELL\\Downloads\\test_dataset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cabba1-6a9d-4a44-a5f2-f9edf15a0d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b374e62-682d-432f-9ac7-25108ef3e219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1073d3a-330b-4f61-b8fd-559c7fc4a759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcced048-e9cb-474a-9c9d-65487000a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = generate_sample_data()\n",
    "\n",
    "# Running Model Evaluation:\n",
    "model = train_sample_model(train_dataset)\n",
    "evaluate_model(model, train_dataset, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8c8a6-f370-4cd0-ba9f-e93d066289b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a7a17-e19a-4df6-9bb0-0ba7a08887e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c6d5b-364f-4ed8-bc52-04cd13ecabe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c2b40-f373-4d99-9046-bee1e3641c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63386288-56c7-4383-af4b-ffb8c3e72a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1737f0-3e04-4c14-bac8-8cb2095bcc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dff753-ea2d-4bab-80b8-f01a99e14e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be3e6a-6490-49ef-8aca-39c13e48381c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430e952-3cc9-413e-b001-7b0b982c9d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16009bf4-0c0f-44b1-aba0-2ecd86739e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be219b00-05d2-408d-ab11-908a515aab55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa8f05-61f4-45fb-a25a-858ea7481124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e4d557-11cb-49b8-a646-45eb0279001e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
