{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d304ccf-6e9b-4701-be9d-bc5ab15670fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333fff95-a654-40c5-8427-fffbd477c094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc5545-b09b-49e0-bc2c-0cdecb4eb884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43526c2c-5afb-4530-8607-f417b769d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"train.csv\"\n",
    "test_path = \"test.csv\"\n",
    "label = \"label\"\n",
    "model_file = \"model.pkl\"\n",
    "#prediction_label_column = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf0d892-830d-4229-8986-0d1a743de015",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IsSingleValue' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define available checks (without parameters)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m check_options \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIsSingleValue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mIsSingleValue\u001b[49m(),\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecialCharacters\u001b[39m\u001b[38;5;124m\"\u001b[39m: SpecialCharacters(),\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixedNulls\u001b[39m\u001b[38;5;124m\"\u001b[39m: MixedNulls(),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixedDataTypes\u001b[39m\u001b[38;5;124m\"\u001b[39m: MixedDataTypes(),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStringMismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: StringMismatch(),  \u001b[38;5;66;03m# Needs parameter\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataDuplicates\u001b[39m\u001b[38;5;124m\"\u001b[39m: DataDuplicates(),\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStringLengthOutOfBounds\u001b[39m\u001b[38;5;124m\"\u001b[39m: StringLengthOutOfBounds(),\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConflictingLabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: ConflictingLabels(),\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutlierSampleDetection\u001b[39m\u001b[38;5;124m\"\u001b[39m: OutlierSampleDetection(),  \u001b[38;5;66;03m# Needs parameter\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureLabelCorrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m: FeatureLabelCorrelation(),\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureFeatureCorrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m: FeatureFeatureCorrelation(),  \u001b[38;5;66;03m# Needs parameter\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdentifierLabelCorrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m: IdentifierLabelCorrelation()\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Checks that require user input parameters\u001b[39;00m\n\u001b[0;32m     18\u001b[0m checks_with_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStringMismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutlierSampleDetection\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_outliers_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutlier_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureFeatureCorrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrelation_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     22\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IsSingleValue' is not defined"
     ]
    }
   ],
   "source": [
    "# Define available checks (without parameters)\n",
    "check_options = {\n",
    "    \"IsSingleValue\": IsSingleValue(),\n",
    "    \"SpecialCharacters\": SpecialCharacters(),\n",
    "    \"MixedNulls\": MixedNulls(),\n",
    "    \"MixedDataTypes\": MixedDataTypes(),\n",
    "    \"StringMismatch\": StringMismatch(),  # Needs parameter\n",
    "    \"DataDuplicates\": DataDuplicates(),\n",
    "    \"StringLengthOutOfBounds\": StringLengthOutOfBounds(),\n",
    "    \"ConflictingLabels\": ConflictingLabels(),\n",
    "    \"OutlierSampleDetection\": OutlierSampleDetection(),  # Needs parameter\n",
    "    \"FeatureLabelCorrelation\": FeatureLabelCorrelation(),\n",
    "    \"FeatureFeatureCorrelation\": FeatureFeatureCorrelation(),  # Needs parameter\n",
    "    \"IdentifierLabelCorrelation\": IdentifierLabelCorrelation()\n",
    "}\n",
    "\n",
    "# Checks that require user input parameters\n",
    "checks_with_params = {\n",
    "    \"StringMismatch\": [\"similarity_threshold\"],\n",
    "    \"OutlierSampleDetection\": [\"max_outliers_ratio\", \"outlier_score_threshold\"],\n",
    "    \"FeatureFeatureCorrelation\": [\"correlation_threshold\"]\n",
    "}\n",
    "\n",
    "# Checks that require user input parameters\n",
    "checks_with_params = {\n",
    "    \"StringMismatch\": {\"similarity_threshold\":5},\n",
    "    \"OutlierSampleDetection\": {\"max_outliers_ratio\": 0.1, \"outlier_score_threshold\":5},\n",
    "    \"FeatureFeatureCorrelation\": {\"correlation_threshold\":0.6}\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9cba29-8a58-4050-9ea4-3eadbc1887db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available Data Integrity Checks:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'check_options' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Display available checks\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAvailable Data Integrity Checks:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mcheck_options\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# User selects which checks to run\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'check_options' is not defined"
     ]
    }
   ],
   "source": [
    "# Display available checks\n",
    "print(\"\\nAvailable Data Integrity Checks:\")\n",
    "for i, test in enumerate(check_options.keys()):\n",
    "    print(f\"{i + 1}. {test}\")\n",
    "\n",
    "# User selects which checks to run\n",
    "selected_indices = input(\"Enter check numbers to run (comma-separated, e.g., 1,3,5): \")\n",
    "selected_indices = [int(idx.strip()) - 1 for idx in selected_indices.split(',') if idx.strip().isdigit()]\n",
    "\n",
    "# Map indices to selected check names\n",
    "selected_tests = [list(check_options.keys())[i] for i in selected_indices]\n",
    "\n",
    "# Get threshold values only for selected tests that require them\n",
    "params = get_threshold_values(selected_tests)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0bcb2e-86e6-4a09-84f1-e8607a1c6406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - You are using deepchecks version 0.19.0, however a newer version is available. Deepchecks is frequently updated with major improvements. You should consider upgrading via the \"python -m pip install --upgrade deepchecks\" command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_DATA_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m train_df \u001b[38;5;241m=\u001b[39m load_csv(\u001b[43mTRAIN_DATA_PATH\u001b[49m)\n\u001b[0;32m     76\u001b[0m test_df \u001b[38;5;241m=\u001b[39m load_csv(TEST_DATA_PATH)    \n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Load or create a sample dataset\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TRAIN_DATA_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "import deepchecks\n",
    "from deepchecks.tabular import Suite\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.checks import (\n",
    "    IsSingleValue, SpecialCharacters, MixedNulls, MixedDataTypes, StringMismatch, DataDuplicates,\n",
    "    StringLengthOutOfBounds, ConflictingLabels, OutlierSampleDetection, FeatureLabelCorrelation,\n",
    "    FeatureFeatureCorrelation, IdentifierLabelCorrelation\n",
    ")\n",
    "\n",
    "def run_deepchecks(data1, data2, label=None):\n",
    "    \"\"\"Run selected deepchecks data integrity tests on the given dataset.\"\"\"\n",
    "    dataset1 = Dataset(data1, label=label)\n",
    "    dataset2 = Dataset(data2, label=label)\n",
    "        \n",
    "    # Create selected checks with conditions where applicable\n",
    "    selected_checks = []\n",
    "    for test in selected_tests:\n",
    "        if test == \"StringMismatch\":\n",
    "            #check = StringMismatch(similarity_threshold=params['similarity_threshold']).add_condition_number_variants_less_or_equal(5)\n",
    "            check = StringMismatch().add_condition_number_variants_less_or_equal(1)\n",
    "        elif test == \"OutlierSampleDetection\":\n",
    "            check = OutlierSampleDetection().add_condition_outlier_ratio_less_or_equal(\n",
    "                params['max_outliers_ratio'], params['outlier_score_threshold']\n",
    "            )\n",
    "        elif test == \"FeatureFeatureCorrelation\":\n",
    "            check = FeatureFeatureCorrelation(correlation_threshold=params['correlation_threshold']).add_condition_max_number_of_pairs_above_threshold(\n",
    "                threshold=params['correlation_threshold'], n_pairs=0\n",
    "            )\n",
    "        elif test == \"IsSingleValue\":\n",
    "            check = IsSingleValue().add_condition_not_single_value()\n",
    "        elif test == \"SpecialCharacters\":\n",
    "            check = SpecialCharacters().add_condition_ratio_of_special_characters_less_or_equal(max_ratio = 0.001)  # No condition available for this check   \n",
    "        elif test == \"MixedNulls\":\n",
    "            check = MixedNulls().add_condition_different_nulls_less_equal_to()\n",
    "        elif test == \"MixedDataTypes\":\n",
    "            check = MixedDataTypes().add_condition_rare_type_ratio_not_in_range((0.01, 0.1))\n",
    "        elif test == \"DataDuplicates\":\n",
    "            check = DataDuplicates().add_condition_ratio_less_or_equal(max_ratio=0.05)\n",
    "        elif test == \"ConflictingLabels\":\n",
    "            check = ConflictingLabels().add_condition_ratio_of_conflicting_labels_less_or_equal(max_ratio=0)\n",
    "        elif test == \"FeatureLabelCorrelation\":\n",
    "            check = FeatureLabelCorrelation().add_condition_feature_pps_less_than(threshold=0.8)\n",
    "        elif test == \"IdentifierLabelCorrelation\":\n",
    "            check = IdentifierLabelCorrelation().add_condition_pps_less_or_equal(max_pps=0)\n",
    "        else:\n",
    "            check = check_options[test]\n",
    "\n",
    "        selected_checks.append(check)\n",
    "    \n",
    "    # Create and run suite\n",
    "    suite = Suite(\"Custom Data Integrity Suite\", *selected_checks)\n",
    "    result = suite.run(dataset1, dataset2)\n",
    "\n",
    "    # Show results\n",
    "    result.show()\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_csv(file_path):\n",
    "    \"\"\"Load CSV file into a Pandas DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Uploaded file is empty.\")\n",
    "        print(f\"Loaded {file_path} with shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    print(\"Loading data...\")\n",
    "    train_df = load_csv(TRAIN_DATA_PATH)\n",
    "    test_df = load_csv(TEST_DATA_PATH)    \n",
    "    # Load or create a sample dataset\n",
    "    \n",
    "    run_deepchecks(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7573c5-8fa9-4b16-9f0a-f7b57210cabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dda54f-39f6-48cd-9754-ca5e5d955990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7c29c-5b9a-4774-9099-f889ed652dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a325d8e-9571-44b4-be6f-6e929450a3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4144fd-5047-4d7c-84e2-eb606d758c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8add4a7b-a97a-421b-a5b9-fa8ebc11862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 2 categorical features were inferred.: feature3, label\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 2 categorical features were inferred.: feature3, label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded train.csv with shape: (1000, 4)\n",
      "Loaded test.csv with shape: (1000, 4)\n",
      "\n",
      "Available checks:\n",
      "1. IsSingleValue\n",
      "2. SpecialCharacters\n",
      "3. MixedNulls\n",
      "4. MixedDataTypes\n",
      "5. StringMismatch\n",
      "6. DataDuplicates\n",
      "7. StringLengthOutOfBounds\n",
      "8. ConflictingLabels\n",
      "9. OutlierSampleDetection\n",
      "10. FeatureLabelCorrelation\n",
      "11. FeatureFeatureCorrelation\n",
      "12. IdentifierLabelCorrelation\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter check numbers to run (comma-separated, e.g., 1,3,5):  1,9\n",
      "Enter max_outliers_ratio for OutlierSampleDetection:  0.1\n",
      "Enter outlier_score_threshold for OutlierSampleDetection:  0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_outliers_ratio': 0.1, 'outlier_score_threshold': 0.9}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2093fd2022334408becaeaeeb9d97737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_R421CEOGRYFUJPFT46L536AKX\">Custom Data Integr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deepchecks\n",
    "from deepchecks.tabular import Suite\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.checks import (\n",
    "    IsSingleValue, SpecialCharacters, MixedNulls, MixedDataTypes, StringMismatch, DataDuplicates,\n",
    "    StringLengthOutOfBounds, ConflictingLabels, OutlierSampleDetection, FeatureLabelCorrelation,\n",
    "    FeatureFeatureCorrelation, IdentifierLabelCorrelation\n",
    ")\n",
    "\n",
    "def get_threshold_values(selected_tests):\n",
    "    \"\"\"Ask for threshold values only for selected tests that require them.\"\"\"\n",
    "    params = {}\n",
    "    for test in selected_tests:\n",
    "        if test in checks_with_params:\n",
    "            for param in checks_with_params[test]:\n",
    "                params[param] = float(input(f\"Enter {param} for {test}: \"))\n",
    "    print(params)\n",
    "    return params\n",
    "\n",
    "def run_deepchecks(data1, data2, label=None):\n",
    "    \"\"\"Run selected deepchecks data integrity tests on the given dataset.\"\"\"\n",
    "    dataset1 = Dataset(data1, label=label)\n",
    "    dataset2 = Dataset(data2, label=label)\n",
    "    # Display available checks\n",
    "    print(\"\\nAvailable checks:\")\n",
    "    for i, test in enumerate(check_options.keys()):\n",
    "        print(f\"{i + 1}. {test}\")\n",
    "\n",
    "    # User selects which checks to run\n",
    "    selected_indices = input(\"Enter check numbers to run (comma-separated, e.g., 1,3,5): \")\n",
    "    selected_indices = [int(idx.strip()) - 1 for idx in selected_indices.split(',') if idx.strip().isdigit()]\n",
    "    \n",
    "    # Map indices to selected check names\n",
    "    selected_tests = [list(check_options.keys())[i] for i in selected_indices]\n",
    "    \n",
    "    # Get threshold values only for selected tests that require them\n",
    "    params = get_threshold_values(selected_tests)\n",
    "    \n",
    "    # Create selected checks with conditions where applicable\n",
    "    selected_checks = []\n",
    "    for test in selected_tests:\n",
    "        if test == \"StringMismatch\":\n",
    "            #check = StringMismatch(similarity_threshold=params['similarity_threshold']).add_condition_number_variants_less_or_equal(5)\n",
    "            check = StringMismatch().add_condition_number_variants_less_or_equal(1)\n",
    "        elif test == \"OutlierSampleDetection\":\n",
    "            check = OutlierSampleDetection().add_condition_outlier_ratio_less_or_equal(\n",
    "                params['max_outliers_ratio'], params['outlier_score_threshold']\n",
    "            )\n",
    "        elif test == \"FeatureFeatureCorrelation\":\n",
    "            check = FeatureFeatureCorrelation(correlation_threshold=params['correlation_threshold']).add_condition_max_number_of_pairs_above_threshold(\n",
    "                threshold=params['correlation_threshold'], n_pairs=0\n",
    "            )\n",
    "        elif test == \"IsSingleValue\":\n",
    "            check = IsSingleValue().add_condition_not_single_value()\n",
    "        elif test == \"SpecialCharacters\":\n",
    "            check = SpecialCharacters().add_condition_ratio_of_special_characters_less_or_equal(max_ratio = 0.001)  # No condition available for this check   \n",
    "        elif test == \"MixedNulls\":\n",
    "            check = MixedNulls().add_condition_different_nulls_less_equal_to()\n",
    "        elif test == \"MixedDataTypes\":\n",
    "            check = MixedDataTypes().add_condition_rare_type_ratio_not_in_range((0.01, 0.1))\n",
    "        elif test == \"DataDuplicates\":\n",
    "            check = DataDuplicates().add_condition_ratio_less_or_equal(max_ratio=0.05)\n",
    "        elif test == \"ConflictingLabels\":\n",
    "            check = ConflictingLabels().add_condition_ratio_of_conflicting_labels_less_or_equal(max_ratio=0)\n",
    "        elif test == \"FeatureLabelCorrelation\":\n",
    "            check = FeatureLabelCorrelation().add_condition_feature_pps_less_than(threshold=0.8)\n",
    "        elif test == \"IdentifierLabelCorrelation\":\n",
    "            check = IdentifierLabelCorrelation().add_condition_pps_less_or_equal(max_pps=0)\n",
    "        else:\n",
    "            check = check_options[test]\n",
    "\n",
    "        selected_checks.append(check)\n",
    "    \n",
    "    # Create and run suite\n",
    "    suite = Suite(\"Custom Data Integrity Suite\", *selected_checks)\n",
    "    result = suite.run(dataset1, dataset2)\n",
    "\n",
    "    # Show results\n",
    "    result.show()\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_csv(file_path):\n",
    "    \"\"\"Load CSV file into a Pandas DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Uploaded file is empty.\")\n",
    "        print(f\"Loaded {file_path} with shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    print(\"Loading data...\")\n",
    "    train_df = load_csv(TRAIN_DATA_PATH)\n",
    "    test_df = load_csv(TEST_DATA_PATH)    \n",
    "    # Load or create a sample dataset\n",
    "    \n",
    "    run_deepchecks(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c767d4d-1ea9-4374-8923-53ae3bd61511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1533f3-c50e-4075-af26-a8d148e354bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa639190-fb2e-450c-b805-49997bf22deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131fefdd-0b6e-4a20-ab55-47ec161a2906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84d125-51c6-4775-8136-fb56e5445139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb60266-b758-46f2-a649-78fd823354b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99a99ab-b010-4162-a617-44c5e243a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchecks\n",
    "#from deepchecks.core.suite import Suite\n",
    "from deepchecks.tabular import Suite\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.checks import (\n",
    "    IsSingleValue, SpecialCharacters, MixedNulls, OutlierSampleDetection\n",
    ")\n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"Get user-defined threshold values for Outlier Sample Detection.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            outlier_score_threshold = float(input(\"Enter Outlier Score Threshold (e.g., 3.0): \"))\n",
    "            max_outliers_ratio = float(input(\"Enter Max Outliers Ratio (e.g., 0.1): \"))\n",
    "            if 0 <= max_outliers_ratio <= 1:\n",
    "                return outlier_score_threshold, max_outliers_ratio\n",
    "            else:\n",
    "                print(\"Max Outliers Ratio must be between 0 and 1.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter numeric values.\")\n",
    "\n",
    "def run_data_integrity(data, label=None):\n",
    "    \"\"\"Run selected deepchecks data integrity tests on the given dataset.\"\"\"\n",
    "    dataset = Dataset(data, label=label)\n",
    "    \n",
    "    # Get threshold values for Outlier Sample Detection\n",
    "    outlier_score_threshold, max_outliers_ratio = get_user_input()\n",
    "    \n",
    "    # Define checks\n",
    "    checks = [\n",
    "        IsSingleValue(),\n",
    "        SpecialCharacters(),\n",
    "        MixedNulls(),\n",
    "        OutlierSampleDetection()\n",
    "        .add_condition_outlier_ratio_less_or_equal(max_outliers_ratio, outlier_score_threshold)\n",
    "    ]\n",
    "    \n",
    "    # Create and run suite\n",
    "    #suite = Suite(\"Custom Data Integrity Suite\", checks)\n",
    "    suite = Suite(\"Custom Data Integrity Suite\", *checks)\n",
    "    result = suite.run(dataset)\n",
    "    \n",
    "    # Show results\n",
    "    result.show()\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage\n",
    "import pandas as pd\n",
    "\n",
    "# Load or create a sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'feature1': [1, 1, 1, 1, 1],  # Single value column\n",
    "    'feature2': ['@hello', 'world!', 'test123', 'data$', 'valid'],  # Special characters\n",
    "    'feature3': [None, 'NULL', 'N/A', '', 'Missing'],  # Mixed nulls\n",
    "    'feature4': [1, 100, 200, -50, 3000]  # Outliers\n",
    "})\n",
    "\n",
    "run_data_integrity(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19dc5eee-5070-44b5-97f7-1d21870bfa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 4 categorical features were inferred.: feature1, feature2, feature3, feature4\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Outlier Score Threshold (e.g., 3.0):  12\n",
      "Enter Max Outliers Ratio (e.g., 0.1):  12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Outliers Ratio must be between 0 and 1.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Outlier Score Threshold (e.g., 3.0):  12\n",
      "Enter Max Outliers Ratio (e.g., 0.1):  12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Outliers Ratio must be between 0 and 1.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Outlier Score Threshold (e.g., 3.0):  12\n",
      "Enter Max Outliers Ratio (e.g., 0.1):  12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Outliers Ratio must be between 0 and 1.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Outlier Score Threshold (e.g., 3.0):  12\n",
      "Enter Max Outliers Ratio (e.g., 0.1):  12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Outliers Ratio must be between 0 and 1.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Outlier Score Threshold (e.g., 3.0):  121\n",
      "Enter Max Outliers Ratio (e.g., 0.1):  12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Outliers Ratio must be between 0 and 1.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Outlier Score Threshold (e.g., 3.0):  3\n",
      "Enter Max Outliers Ratio (e.g., 0.1):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input. Please enter numeric values.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Outlier Score Threshold (e.g., 3.0):  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load or create a sample dataset\u001b[39;00m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature1\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],  \u001b[38;5;66;03m# Single value column\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature2\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@hello\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworld!\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest123\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Special characters\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature3\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNULL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Mixed nulls\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature4\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m3000\u001b[39m]  \u001b[38;5;66;03m# Outliers\u001b[39;00m\n\u001b[0;32m     10\u001b[0m })\n\u001b[1;32m---> 12\u001b[0m \u001b[43mrun_data_integrity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m, in \u001b[0;36mrun_data_integrity\u001b[1;34m(data, label)\u001b[0m\n\u001b[0;32m     24\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset(data, label\u001b[38;5;241m=\u001b[39mlabel)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Get threshold values for Outlier Sample Detection\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m outlier_score_threshold, max_outliers_ratio \u001b[38;5;241m=\u001b[39m \u001b[43mget_user_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Define checks\u001b[39;00m\n\u001b[0;32m     30\u001b[0m checks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     31\u001b[0m     IsSingleValue(),\n\u001b[0;32m     32\u001b[0m     SpecialCharacters(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;241m.\u001b[39madd_condition_outlier_ratio_less_or_equal(max_outliers_ratio, outlier_score_threshold)\n\u001b[0;32m     36\u001b[0m ]\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mget_user_input\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     outlier_score_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter Outlier Score Threshold (e.g., 3.0): \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 14\u001b[0m     max_outliers_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter Max Outliers Ratio (e.g., 0.1): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_outliers_ratio \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outlier_score_threshold, max_outliers_ratio\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "import pandas as pd\n",
    "\n",
    "# Load or create a sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'feature1': [1, 1, 1, 1, 1],  # Single value column\n",
    "    'feature2': ['@hello', 'world!', 'test123', 'data$', 'valid'],  # Special characters\n",
    "    'feature3': [None, 'NULL', 'N/A', '', 'Missing'],  # Mixed nulls\n",
    "    'feature4': [1, 100, 200, -50, 3000]  # Outliers\n",
    "})\n",
    "\n",
    "run_data_integrity(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9322b68e-4604-4864-bc30-2de318d116ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 4 categorical features were inferred.: feature1, feature2, feature3, feature4\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Outlier Score Threshold (e.g., 3.0):  3\n",
      "Enter Max Outliers Ratio (e.g., 0.1):  0.2\n",
      "Enter Similarity Threshold for String Mismatch (e.g., 0.8):  0.8\n",
      "Enter Correlation Threshold for Feature-Feature Correlation (e.g., 0.9):  0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available checks:\n",
      "1. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.is_single_value.IsSingleValue'>>\n",
      "2. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.special_chars.SpecialCharacters'>>\n",
      "3. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.mixed_nulls.MixedNulls'>>\n",
      "4. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.mixed_data_types.MixedDataTypes'>>\n",
      "5. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.string_mismatch.StringMismatch'>>\n",
      "6. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.data_duplicates.DataDuplicates'>>\n",
      "7. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.string_length_out_of_bounds.StringLengthOutOfBounds'>>\n",
      "8. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.conflicting_labels.ConflictingLabels'>>\n",
      "9. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.outlier_sample_detection.OutlierSampleDetection'>>\n",
      "10. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.feature_label_correlation.FeatureLabelCorrelation'>>\n",
      "11. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.feature_feature_correlation.FeatureFeatureCorrelation'>>\n",
      "12. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.identifier_label_correlation.IdentifierLabelCorrelation'>>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter check numbers to run (comma-separated, e.g., 1,3,5):  1, 2, 6, 10, 11,12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce6329d0648476681cb0398cb253a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_PNFX1MR90FOLXZXPJEY698F8Y\">Custom Data Integr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deepchecks\n",
    "from deepchecks.tabular import Suite\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.checks import (\n",
    "    IsSingleValue, SpecialCharacters, MixedNulls, MixedDataTypes, StringMismatch, DataDuplicates,\n",
    "    StringLengthOutOfBounds, ConflictingLabels, OutlierSampleDetection, FeatureLabelCorrelation,\n",
    "    FeatureFeatureCorrelation, IdentifierLabelCorrelation\n",
    ")\n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"Get user-defined parameter values for checks that require input.\"\"\"\n",
    "    params = {}\n",
    "    params['outlier_score_threshold'] = float(input(\"Enter Outlier Score Threshold (e.g., 3.0): \"))\n",
    "    params['max_outliers_ratio'] = float(input(\"Enter Max Outliers Ratio (e.g., 0.1): \"))\n",
    "    params['similarity_threshold'] = float(input(\"Enter Similarity Threshold for String Mismatch (e.g., 0.8): \"))\n",
    "    params['correlation_threshold'] = float(input(\"Enter Correlation Threshold for Feature-Feature Correlation (e.g., 0.9): \"))\n",
    "    return params\n",
    "\n",
    "def run_deepchecks(data, label=None):\n",
    "    \"\"\"Run selected deepchecks data integrity tests on the given dataset.\"\"\"\n",
    "    dataset = Dataset(data, label=label)\n",
    "    \n",
    "    # Get threshold values for checks requiring parameters\n",
    "    params = get_user_input()\n",
    "    \n",
    "    # Define checks with input parameters where applicable\n",
    "    checks = [\n",
    "        IsSingleValue(),\n",
    "        SpecialCharacters(),\n",
    "        MixedNulls(),\n",
    "        MixedDataTypes(),\n",
    "        StringMismatch(similarity_threshold=params['similarity_threshold']),\n",
    "        DataDuplicates(),\n",
    "        StringLengthOutOfBounds(),\n",
    "        ConflictingLabels(),\n",
    "        OutlierSampleDetection()\n",
    "        .add_condition_outlier_ratio_less_or_equal(params['max_outliers_ratio'], params['outlier_score_threshold']),\n",
    "        FeatureLabelCorrelation(),\n",
    "        FeatureFeatureCorrelation(correlation_threshold=params['correlation_threshold']),\n",
    "        IdentifierLabelCorrelation()\n",
    "    ]\n",
    "    \n",
    "    # Allow user to select which checks to run\n",
    "    print(\"Available checks:\")\n",
    "    for i, check in enumerate(checks):\n",
    "        print(f\"{i + 1}. {check.name}\")\n",
    "    selected_indices = input(\"Enter check numbers to run (comma-separated, e.g., 1,3,5): \")\n",
    "    selected_indices = [int(idx.strip()) - 1 for idx in selected_indices.split(',') if idx.strip().isdigit()]\n",
    "    \n",
    "    selected_checks = [checks[i] for i in selected_indices]\n",
    "    \n",
    "    # Create and run suite\n",
    "    suite = Suite(\"Custom Data Integrity Suite\", *selected_checks)\n",
    "    result = suite.run(dataset)\n",
    "    \n",
    "    # Show results\n",
    "    result.show()\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load or create a sample dataset\n",
    "    data = pd.DataFrame({\n",
    "        'feature1': [1, 1, 1, 1, 1],  # Single value column\n",
    "        'feature2': ['@hello', 'world!', 'test123', 'data$', 'valid'],  # Special characters\n",
    "        'feature3': [None, 'NULL', 'N/A', '', 'Missing'],  # Mixed nulls\n",
    "        'feature4': [1, 100, 200, -50, 3000]  # Outliers\n",
    "    })\n",
    "    \n",
    "    run_deepchecks(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016643f7-97e4-4e5a-b4d7-fc2c1f9b50c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 4 categorical features were inferred.: feature1, feature2, feature3, feature4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available checks:\n",
      "1. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.is_single_value.IsSingleValue'>>\n",
      "2. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.special_chars.SpecialCharacters'>>\n",
      "3. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.mixed_nulls.MixedNulls'>>\n",
      "4. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.mixed_data_types.MixedDataTypes'>>\n",
      "5. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.string_mismatch.StringMismatch'>>\n",
      "6. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.data_duplicates.DataDuplicates'>>\n",
      "7. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.string_length_out_of_bounds.StringLengthOutOfBounds'>>\n",
      "8. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.conflicting_labels.ConflictingLabels'>>\n",
      "9. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.outlier_sample_detection.OutlierSampleDetection'>>\n",
      "10. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.feature_label_correlation.FeatureLabelCorrelation'>>\n",
      "11. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.feature_feature_correlation.FeatureFeatureCorrelation'>>\n",
      "12. <bound method BaseCheck.name of <class 'deepchecks.tabular.checks.data_integrity.identifier_label_correlation.IdentifierLabelCorrelation'>>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter check numbers to run (comma-separated, e.g., 1,3,5):  1,2,3,6,10,11,12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2df9078a334cf3be77f5f937bfd778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_47BJZPSH2RTN37TJUR3DKGUZ4\">Custom Data Integr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deepchecks\n",
    "from deepchecks.tabular import Suite\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.checks import (\n",
    "    IsSingleValue, SpecialCharacters, MixedNulls, MixedDataTypes, StringMismatch, DataDuplicates,\n",
    "    StringLengthOutOfBounds, ConflictingLabels, OutlierSampleDetection, FeatureLabelCorrelation,\n",
    "    FeatureFeatureCorrelation, IdentifierLabelCorrelation\n",
    ")\n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"Get user-defined parameter values for checks that require input.\"\"\"\n",
    "    params = {}\n",
    "    params['outlier_score_threshold'] = 3.0 #float(input(\"Enter Outlier Score Threshold (e.g., 3.0): \"))\n",
    "    params['max_outliers_ratio'] = 0.1 #float(input(\"Enter Max Outliers Ratio (e.g., 0.1): \"))\n",
    "    params['similarity_threshold'] = 0.8 #float(input(\"Enter Similarity Threshold for String Mismatch (e.g., 0.8): \"))\n",
    "    params['correlation_threshold'] = 0.9 #float(input(\"Enter Correlation Threshold for Feature-Feature Correlation (e.g., 0.9): \"))\n",
    "    return params\n",
    "\n",
    "def run_deepchecks(data, label=None):\n",
    "    \"\"\"Run selected deepchecks data integrity tests on the given dataset.\"\"\"\n",
    "    dataset = Dataset(data, label=label)\n",
    "    \n",
    "    # Get threshold values for checks requiring parameters\n",
    "    params = get_user_input()\n",
    "    \n",
    "    # Define checks with valid conditions where applicable\n",
    "    checks = [\n",
    "        IsSingleValue().add_condition_not_single_value(),  # Ensure column is not a single value\n",
    "        SpecialCharacters().add_condition_ratio_of_special_characters_less_or_equal(max_ratio = 0.001),  # No condition available for this check\n",
    "        MixedNulls().add_condition_different_nulls_less_equal_to(), #add_condition_no_mixed_nulls(),  # Ensure no mixed null types\n",
    "        MixedDataTypes().add_condition_rare_type_ratio_not_in_range(), #add_condition_no_mixed_types(),  # Ensure no mixed data types\n",
    "        StringMismatch(similarity_threshold=params['similarity_threshold']).add_condition_number_variants_less_or_equal(5), #add_condition_number_of_mismatches_less_or_equal(5),  # Allow max 5 mismatches\n",
    "        DataDuplicates().add_condition_ratio_less_or_equal(max_ratio = 0.05), #add_condition_no_duplicates(),  # Ensure no duplicate rows\n",
    "        StringLengthOutOfBounds(),  # No condition available for this check\n",
    "        ConflictingLabels().add_condition_ratio_of_conflicting_labels_less_or_equal(max_ratio=0), #.add_condition_no_conflicting_labels(),  # Ensure no conflicting labels\n",
    "        OutlierSampleDetection().add_condition_outlier_ratio_less_or_equal(params['max_outliers_ratio'], params['outlier_score_threshold']),  # Outliers within limits\n",
    "        FeatureLabelCorrelation().add_condition_feature_pps_less_than(threshold = 0.8), #.add_condition_feature_label_correlation_less_or_equal(0.9),  # Max correlation 0.9\n",
    "        FeatureFeatureCorrelation(correlation_threshold=params['correlation_threshold']).add_condition_max_number_of_pairs_above_threshold(threshold = 0.9, n_pairs = 0), #.add_condition_feature_feature_correlation_less_or_equal(params['correlation_threshold']),  # Correlation within threshold\n",
    "        IdentifierLabelCorrelation().add_condition_pps_less_or_equal(max_pps = 0) #add_condition_identifier_label_correlation_less_or_equal(0.8)  # Max identifier correlation 0.8\n",
    "    ]\n",
    "    \n",
    "    # Allow user to select which checks to run\n",
    "    print(\"Available checks:\")\n",
    "    for i, check in enumerate(checks):\n",
    "        print(f\"{i + 1}. {check.name}\")\n",
    "    selected_indices = input(\"Enter check numbers to run (comma-separated, e.g., 1,3,5): \")\n",
    "    selected_indices = [int(idx.strip()) - 1 for idx in selected_indices.split(',') if idx.strip().isdigit()]\n",
    "    \n",
    "    selected_checks = [checks[i] for i in selected_indices]\n",
    "    \n",
    "    # Create and run suite\n",
    "    suite = Suite(\"Custom Data Integrity Suite\", *selected_checks)\n",
    "    result = suite.run(dataset)\n",
    "    \n",
    "    # Show results\n",
    "    result.show()\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load or create a sample dataset\n",
    "    data = pd.DataFrame({\n",
    "        'feature1': [1, 1, 1, 1, 1],  # Single value column\n",
    "        'feature2': ['@hello', 'world!', 'test123', 'data$', 'valid'],  # Special characters\n",
    "        'feature3': [None, 'NULL', 'N/A', '', 'Missing'],  # Mixed nulls\n",
    "        'feature4': [1, 100, 200, -50, 3000]  # Outliers\n",
    "    })\n",
    "    \n",
    "    run_deepchecks(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499917d1-b7b4-4977-9d32-75f116a8752b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f7eb3-fd24-4a62-b62f-00b2208eba51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955f59c-4d21-4265-80cd-259b5683b1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10633c5d-7315-45f7-bcbb-261d7f840b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64641db2-d9e9-483c-9a01-dba874bcd51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb1227-7d7b-4043-82a2-1353c2df23ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d39c9-388d-45b0-8dcd-6ffd288f7053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950b5b2-a719-49c3-a544-65bce07d33f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9686a-1f38-48d0-8810-e70fc7e5fffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2466f1f0-d93a-49ce-8bad-4123946aa4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cceb7fb2-9b9c-4893-ae9d-f6970770d014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 4 categorical features were inferred.: feature1, feature2, feature3, feature4\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 4 categorical features were inferred.: feature1, feature2, feature3, feature4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available checks:\n",
      "1. IsSingleValue\n",
      "2. SpecialCharacters\n",
      "3. MixedNulls\n",
      "4. MixedDataTypes\n",
      "5. StringMismatch\n",
      "6. DataDuplicates\n",
      "7. StringLengthOutOfBounds\n",
      "8. ConflictingLabels\n",
      "9. OutlierSampleDetection\n",
      "10. FeatureLabelCorrelation\n",
      "11. FeatureFeatureCorrelation\n",
      "12. IdentifierLabelCorrelation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 116\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Load or create a sample dataset\u001b[39;00m\n\u001b[0;32m    109\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature1\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],  \u001b[38;5;66;03m# Single value column\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature2\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@hello\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworld!\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest123\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Special characters\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature3\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNULL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Mixed nulls\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature4\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m3000\u001b[39m]  \u001b[38;5;66;03m# Outliers\u001b[39;00m\n\u001b[0;32m    114\u001b[0m })\n\u001b[1;32m--> 116\u001b[0m \u001b[43mrun_deepchecks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m, in \u001b[0;36mrun_deepchecks\u001b[1;34m(data1, data2, label)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# User selects which checks to run\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m selected_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter check numbers to run (comma-separated, e.g., 1,3,5): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m selected_indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(idx\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m selected_indices\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39misdigit()]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Map indices to selected check names\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter check numbers to run (comma-separated, e.g., 1,3,5):  1\n"
     ]
    }
   ],
   "source": [
    "import deepchecks\n",
    "from deepchecks.tabular import Suite\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.checks import (\n",
    "    IsSingleValue, SpecialCharacters, MixedNulls, MixedDataTypes, StringMismatch, DataDuplicates,\n",
    "    StringLengthOutOfBounds, ConflictingLabels, OutlierSampleDetection, FeatureLabelCorrelation,\n",
    "    FeatureFeatureCorrelation, IdentifierLabelCorrelation\n",
    ")\n",
    "\n",
    "# Define available checks (without parameters)\n",
    "check_options = {\n",
    "    \"IsSingleValue\": IsSingleValue(),\n",
    "    \"SpecialCharacters\": SpecialCharacters(),\n",
    "    \"MixedNulls\": MixedNulls(),\n",
    "    \"MixedDataTypes\": MixedDataTypes(),\n",
    "    \"StringMismatch\": StringMismatch(),  # Needs parameter\n",
    "    \"DataDuplicates\": DataDuplicates(),\n",
    "    \"StringLengthOutOfBounds\": StringLengthOutOfBounds(),\n",
    "    \"ConflictingLabels\": ConflictingLabels(),\n",
    "    \"OutlierSampleDetection\": OutlierSampleDetection(),  # Needs parameter\n",
    "    \"FeatureLabelCorrelation\": FeatureLabelCorrelation(),\n",
    "    \"FeatureFeatureCorrelation\": FeatureFeatureCorrelation(),  # Needs parameter\n",
    "    \"IdentifierLabelCorrelation\": IdentifierLabelCorrelation()\n",
    "}\n",
    "\n",
    "# Checks that require user input parameters\n",
    "checks_with_params = {\n",
    "    \"StringMismatch\": [\"similarity_threshold\"],\n",
    "    \"OutlierSampleDetection\": [\"max_outliers_ratio\", \"outlier_score_threshold\"],\n",
    "    \"FeatureFeatureCorrelation\": [\"correlation_threshold\"]\n",
    "}\n",
    "\n",
    "def get_threshold_values(selected_tests):\n",
    "    \"\"\"Ask for threshold values only for selected tests that require them.\"\"\"\n",
    "    params = {}\n",
    "    for test in selected_tests:\n",
    "        if test in checks_with_params:\n",
    "            for param in checks_with_params[test]:\n",
    "                params[param] = float(input(f\"Enter {param} for {test}: \"))\n",
    "    return params\n",
    "\n",
    "def run_deepchecks(data1, data2, label=None):\n",
    "    \"\"\"Run selected deepchecks data integrity tests on the given dataset.\"\"\"\n",
    "    dataset1 = Dataset(data1, label=label)\n",
    "    dataset2 = Dataset(data2, label=label)\n",
    "    # Display available checks\n",
    "    print(\"\\nAvailable checks:\")\n",
    "    for i, test in enumerate(check_options.keys()):\n",
    "        print(f\"{i + 1}. {test}\")\n",
    "\n",
    "    # User selects which checks to run\n",
    "    selected_indices = input(\"Enter check numbers to run (comma-separated, e.g., 1,3,5): \")\n",
    "    selected_indices = [int(idx.strip()) - 1 for idx in selected_indices.split(',') if idx.strip().isdigit()]\n",
    "    \n",
    "    # Map indices to selected check names\n",
    "    selected_tests = [list(check_options.keys())[i] for i in selected_indices]\n",
    "    \n",
    "    # Get threshold values only for selected tests that require them\n",
    "    params = get_threshold_values(selected_tests)\n",
    "    \n",
    "    # Create selected checks with conditions where applicable\n",
    "    selected_checks = []\n",
    "    for test in selected_tests:\n",
    "        if test == \"StringMismatch\":\n",
    "            #check = StringMismatch(similarity_threshold=params['similarity_threshold']).add_condition_number_variants_less_or_equal(5)\n",
    "            check = StringMismatch().add_condition_number_variants_less_or_equal(1)\n",
    "        elif test == \"OutlierSampleDetection\":\n",
    "            check = OutlierSampleDetection().add_condition_outlier_ratio_less_or_equal(\n",
    "                params['max_outliers_ratio'], params['outlier_score_threshold']\n",
    "            )\n",
    "        elif test == \"FeatureFeatureCorrelation\":\n",
    "            check = FeatureFeatureCorrelation(correlation_threshold=params['correlation_threshold']).add_condition_max_number_of_pairs_above_threshold(\n",
    "                threshold=params['correlation_threshold'], n_pairs=0\n",
    "            )\n",
    "        elif test == \"IsSingleValue\":\n",
    "            check = IsSingleValue().add_condition_not_single_value()\n",
    "        elif test == \"SpecialCharacters\":\n",
    "            check = SpecialCharacters().add_condition_ratio_of_special_characters_less_or_equal(max_ratio = 0.001)  # No condition available for this check   \n",
    "        elif test == \"MixedNulls\":\n",
    "            check = MixedNulls().add_condition_different_nulls_less_equal_to()\n",
    "        elif test == \"MixedDataTypes\":\n",
    "            check = MixedDataTypes().add_condition_rare_type_ratio_not_in_range((0.01, 0.1))\n",
    "        elif test == \"DataDuplicates\":\n",
    "            check = DataDuplicates().add_condition_ratio_less_or_equal(max_ratio=0.05)\n",
    "        elif test == \"ConflictingLabels\":\n",
    "            check = ConflictingLabels().add_condition_ratio_of_conflicting_labels_less_or_equal(max_ratio=0)\n",
    "        elif test == \"FeatureLabelCorrelation\":\n",
    "            check = FeatureLabelCorrelation().add_condition_feature_pps_less_than(threshold=0.8)\n",
    "        elif test == \"IdentifierLabelCorrelation\":\n",
    "            check = IdentifierLabelCorrelation().add_condition_pps_less_or_equal(max_pps=0)\n",
    "        else:\n",
    "            check = check_options[test]\n",
    "\n",
    "        selected_checks.append(check)\n",
    "    \n",
    "    # Create and run suite\n",
    "    suite = Suite(\"Custom Data Integrity Suite\", *selected_checks)\n",
    "    result = suite.run(dataset1, dataset2)\n",
    "\n",
    "    # Show results\n",
    "    result.show()\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load or create a sample dataset\n",
    "    data = pd.DataFrame({\n",
    "        'feature1': [1, 1, 1, 1, 1],  # Single value column\n",
    "        'feature2': ['@hello', 'world!', 'test123', 'data$', 'valid'],  # Special characters\n",
    "        'feature3': [None, 'NULL', 'N/A', '', 'Missing'],  # Mixed nulls\n",
    "        'feature4': [1, 100, 200, -50, 3000]  # Outliers\n",
    "    })\n",
    "    \n",
    "    run_deepchecks(data, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "122914b5-dfc9-4aeb-ba29-bd4a70e2f957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - Received a \"pandas.DataFrame\" instance. It is recommended to pass a \"deepchecks.tabular.Dataset\" instance by initializing it with the data and metadata, for example by doing \"Dataset(dataframe, label=label, cat_features=cat_features)\"\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 4 categorical features were inferred.: feature1, feature2, feature3, feature4\n",
      "deepchecks - WARNING - Received a \"pandas.DataFrame\" instance. It is recommended to pass a \"deepchecks.tabular.Dataset\" instance by initializing it with the data and metadata, for example by doing \"Dataset(dataframe, label=label, cat_features=cat_features)\"\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 4 categorical features were inferred.: feature1, feature2, feature3, feature4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92d9d86f0134e8b9f261c45991c6d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_7113MY6JJE48JUYJ5EBI12N59\">Data Integrity Sui…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.DataFrame({\n",
    "        'feature1': [1, 1, 1, 1, 1],  # Single value column\n",
    "        'feature2': ['@hello', 'world!', 'test123', 'data$', 'valid'],  # Special characters\n",
    "        'feature3': [None, 'NULL', 'N/A', '', 'Missing'],  # Mixed nulls\n",
    "        'feature4': [1, 100, 200, -50, 3000]  # Outliers\n",
    "    })\n",
    "\n",
    "from deepchecks.tabular.suites import data_integrity\n",
    "suite = data_integrity()\n",
    "result = suite.run(train_dataset = data, test_dataset = data)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c051d-b9c6-4fa2-a590-f688637db6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbffc8a-5b15-4693-8851-993441e90354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c768fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6974091e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1733ae2c0841e3ad71f47839a22799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Suite:', options=('Data Integrity', 'Train-Test Validation', 'Model Evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import deepchecks\n",
    "from deepchecks.tabular.suites import data_integrity, train_test_validation, model_evaluation\n",
    "from ipywidgets import interactive, VBox, HBox, Dropdown, FloatSlider, Button, Checkbox, Output, Label\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from docx import Document  # For saving the results to a Word document\n",
    "from datetime import datetime  # To generate unique filenames\n",
    "\n",
    "# -------------------- Step 1: Create Sample Data (Optional) --------------------\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------- Step 2: Train a Simple Model (Optional) --------------------\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# -------------------- Step 3: Create Jupyter Widgets --------------------\n",
    "available_suites = {\n",
    "    'Data Integrity': data_integrity(),\n",
    "    'Train-Test Validation': train_test_validation(),\n",
    "    'Model Evaluation': model_evaluation()\n",
    "}\n",
    "\n",
    "# Dropdown to select the suite\n",
    "suite_dropdown = Dropdown(\n",
    "    options=list(available_suites.keys()),\n",
    "    value='Data Integrity',\n",
    "    description='Suite:'\n",
    ")\n",
    "\n",
    "# Checkboxes and sliders for each test (Populated dynamically)\n",
    "test_checkboxes = {}\n",
    "threshold_sliders = {}\n",
    "test_selection_box = VBox([])  # This will hold both checkboxes and sliders for each test\n",
    "\n",
    "# Button to run the suite\n",
    "run_button = Button(description=\"Run Suite\")\n",
    "\n",
    "# Output to display the suite results\n",
    "output = Output()\n",
    "\n",
    "# -------------------- Step 4: Update Test Selection When Suite Changes --------------------\n",
    "def update_test_selection(change):\n",
    "    \"\"\"Update the available tests (checkboxes and sliders) when the suite changes.\"\"\"\n",
    "    global test_checkboxes, threshold_sliders\n",
    "    selected_suite = change['new']\n",
    "    \n",
    "    # Clear old checkboxes and sliders\n",
    "    test_checkboxes.clear()\n",
    "    threshold_sliders.clear()\n",
    "    \n",
    "    # Create new checkboxes and sliders for each test in the selected suite\n",
    "    test_widgets = []\n",
    "    for test_key, test_obj in available_suites[selected_suite].checks.items():\n",
    "        # Create a checkbox for the test\n",
    "        test_checkboxes[test_obj.name()] = Checkbox(\n",
    "            value=True, \n",
    "            description=test_obj.name()\n",
    "        )\n",
    "        \n",
    "        # Create a slider for the threshold of this test\n",
    "        threshold_sliders[test_obj.name()] = FloatSlider(\n",
    "            value=0.8, \n",
    "            min=0.0, \n",
    "            max=1.0, \n",
    "            step=0.05, \n",
    "            description='Threshold'\n",
    "        )\n",
    "        \n",
    "        # Add the checkbox and slider side-by-side (HBox)\n",
    "        test_widgets.append(HBox([test_checkboxes[test_obj.name()], threshold_sliders[test_obj.name()]]))\n",
    "    \n",
    "    # Update the test selection box with all the new checkboxes and sliders\n",
    "    test_selection_box.children = test_widgets\n",
    "\n",
    "# Call this function once at the start to initialize the UI\n",
    "update_test_selection({'new': 'Data Integrity'})\n",
    "\n",
    "# Attach the function to observe changes in the dropdown selection\n",
    "suite_dropdown.observe(update_test_selection, names='value')\n",
    "\n",
    "# -------------------- Step 5: Run the Selected Suite and Save Results --------------------\n",
    "def run_selected_suite(b):\n",
    "    \"\"\"Run the selected suite with selected tests and thresholds, then save the results to a Word file.\"\"\"\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        \n",
    "        selected_suite_name = suite_dropdown.value\n",
    "        selected_suite = available_suites[selected_suite_name]\n",
    "        \n",
    "        # Filter the selected tests\n",
    "        selected_tests = [test for test in selected_suite.checks if test_checkboxes[test[1].name()].value]\n",
    "        \n",
    "        # Set the custom threshold for each selected test\n",
    "        for test in selected_tests:\n",
    "            test_name = test[1].name()\n",
    "            print(\"Set the custom threshold for each selected test \", test_name)\n",
    "            threshold_value = threshold_sliders[test_name].value\n",
    "            test.add_condition_test_score_greater_than(threshold_value)\n",
    "        \n",
    "        # Run the selected suite\n",
    "        if selected_suite_name == 'Data Integrity':\n",
    "            result = selected_suite.run(X_train)\n",
    "        elif selected_suite_name == 'Train-Test Validation':\n",
    "            result = selected_suite.run(X_train, X_test, y_train, y_test)\n",
    "        elif selected_suite_name == 'Model Evaluation':\n",
    "            result = selected_suite.run(X_train, y_train, X_test, y_test, model)\n",
    "        \n",
    "        result.show()\n",
    "        \n",
    "        # -------------------- Save the Results to a Word File --------------------\n",
    "        try:\n",
    "            # Create a new Word document\n",
    "            doc = Document()\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            \n",
    "            doc.add_heading(f'Deepchecks Report - {selected_suite_name}', level=1)\n",
    "            doc.add_paragraph(f'Run Timestamp: {timestamp}\\n')\n",
    "            \n",
    "            summary = result.to_dataframe()\n",
    "            doc.add_heading('Test Summary', level=2)\n",
    "            \n",
    "            for index, row in summary.iterrows():\n",
    "                doc.add_paragraph(f'Test: {row[\"Check\"]} | Status: {row[\"Result\"]} | Value: {row[\"Value\"]}')\n",
    "            \n",
    "            doc.add_heading('Detailed Results', level=2)\n",
    "            for check_result in result.get_not_passed_checks():\n",
    "                doc.add_paragraph(f'Check Name: {check_result.name}')\n",
    "                doc.add_paragraph(f'Description: {check_result.description}')\n",
    "                doc.add_paragraph(f'Result: {check_result.result}')\n",
    "                doc.add_paragraph(f'Message: {check_result.display()}')\n",
    "                doc.add_paragraph('---' * 10)\n",
    "            \n",
    "            file_name = f'{selected_suite_name}_Report_{timestamp}.docx'\n",
    "            doc.save(file_name)\n",
    "            output.append_stdout(f\"Results saved to {file_name}\\n\")\n",
    "        except Exception as e:\n",
    "            output.append_stdout(f\"Error saving results: {str(e)}\\n\")\n",
    "\n",
    "# Attach the \"run suite\" button to the handler function\n",
    "run_button.on_click(run_selected_suite)\n",
    "\n",
    "# -------------------- Step 6: Display the Jupyter Widget UI --------------------\n",
    "# Layout for the widget\n",
    "ui = VBox([\n",
    "    suite_dropdown,  # Dropdown to select the suite\n",
    "    test_selection_box,  # Box containing test checkboxes and sliders\n",
    "    run_button,  # Button to run the suite\n",
    "    output  # Output display for test results and file save status\n",
    "])\n",
    "\n",
    "# Display the interactive UI\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f8fb4-084b-4294-b434-a56e7ba513fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c487c-5f9c-43b0-9e3d-601e1b05adbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dee5ea2-305e-48ce-9b14-fec2cb905642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b6a57a47ee4308af6686c7bf38cd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Suite:', options=('Data Integrity', 'Train-Test Validation', 'Model Evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import deepchecks\n",
    "from deepchecks.tabular.suites import data_integrity, train_test_validation, model_evaluation\n",
    "from ipywidgets import VBox, HBox, Dropdown, FloatSlider, Button, Checkbox, Output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from docx import Document  # For saving results\n",
    "from datetime import datetime  # For unique filenames\n",
    "\n",
    "# -------------------- Step 1: Create Sample Data --------------------\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "from deepchecks.tabular import Dataset\n",
    "\n",
    "# Convert NumPy arrays to Deepchecks Dataset\n",
    "train_dataset = Dataset(pd.DataFrame(X_train), label=pd.Series(y_train))\n",
    "test_dataset = Dataset(pd.DataFrame(X_test), label=pd.Series(y_test))\n",
    "\n",
    "\n",
    "# -------------------- Step 2: Train a Simple Model --------------------\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# -------------------- Step 3: Create Jupyter Widgets --------------------\n",
    "available_suites = {\n",
    "    'Data Integrity': data_integrity(),\n",
    "    'Train-Test Validation': train_test_validation(),\n",
    "    'Model Evaluation': model_evaluation()\n",
    "}\n",
    "\n",
    "# Dropdown to select the suite\n",
    "suite_dropdown = Dropdown(\n",
    "    options=list(available_suites.keys()),\n",
    "    value='Data Integrity',\n",
    "    description='Suite:'\n",
    ")\n",
    "\n",
    "# Checkboxes and sliders for each test (Populated dynamically)\n",
    "test_checkboxes = {}\n",
    "threshold_sliders = {}\n",
    "test_selection_box = VBox([])\n",
    "\n",
    "# Button to run the suite\n",
    "run_button = Button(description=\"Run Suite\")\n",
    "\n",
    "# Output to display results\n",
    "output = Output()\n",
    "\n",
    "# -------------------- Step 4: Update Test Selection When Suite Changes --------------------\n",
    "def update_test_selection(change):\n",
    "    \"\"\"Update available tests (checkboxes and sliders) when the suite changes.\"\"\"\n",
    "    global test_checkboxes, threshold_sliders\n",
    "    selected_suite = change['new']\n",
    "    \n",
    "    # Clear old checkboxes and sliders\n",
    "    test_checkboxes.clear()\n",
    "    threshold_sliders.clear()\n",
    "    \n",
    "    test_widgets = []\n",
    "    for test_key, test_obj in available_suites[selected_suite].checks.items():\n",
    "        test_name = test_obj.name()\n",
    "        \n",
    "        # Create a checkbox for the test\n",
    "        test_checkboxes[test_name] = Checkbox(value=True, description=test_name)\n",
    "        \n",
    "        # Add threshold sliders only for relevant tests\n",
    "        slider_widget = None\n",
    "        if test_name == \"Outlier Sample Detection\":\n",
    "            # Needs two sliders for max_outliers_ratio and outlier_score_threshold\n",
    "            max_outliers_slider = FloatSlider(value=0.01, min=0.0, max=0.1, step=0.001, description='Max Outliers')\n",
    "            score_threshold_slider = FloatSlider(value=0.9, min=0.0, max=1.0, step=0.05, description='Score Threshold')\n",
    "            threshold_sliders[test_name] = (max_outliers_slider, score_threshold_slider)\n",
    "            slider_widget = HBox([max_outliers_slider, score_threshold_slider])\n",
    "        \n",
    "        elif hasattr(test_obj, \"add_condition_greater_than\"):\n",
    "            # Generic case for tests that support 'add_condition_greater_than'\n",
    "            threshold_sliders[test_name] = FloatSlider(value=0.8, min=0.0, max=1.0, step=0.05, description='Threshold')\n",
    "            slider_widget = threshold_sliders[test_name]\n",
    "\n",
    "        row_widgets = [test_checkboxes[test_name]]\n",
    "        if slider_widget:\n",
    "            row_widgets.append(slider_widget)\n",
    "        \n",
    "        test_widgets.append(HBox(row_widgets))\n",
    "    \n",
    "    test_selection_box.children = test_widgets\n",
    "\n",
    "update_test_selection({'new': 'Data Integrity'})  # Initialize UI\n",
    "suite_dropdown.observe(update_test_selection, names='value')\n",
    "\n",
    "# -------------------- Step 5: Run the Selected Suite and Save Results --------------------\n",
    "def run_selected_suite(b):\n",
    "    \"\"\"Run the selected suite with thresholds and save results to a Word file.\"\"\"\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        \n",
    "        selected_suite_name = suite_dropdown.value\n",
    "        selected_suite = available_suites[selected_suite_name]\n",
    "        \n",
    "        # Filter selected tests\n",
    "        selected_tests = [test for test in selected_suite.checks.items() if test_checkboxes[test[1].name()].value]\n",
    "        \n",
    "        # Set custom thresholds for selected tests\n",
    "        for test in selected_tests:\n",
    "            test_name = test[1].name()\n",
    "            try:\n",
    "                if test_name == \"Outlier Sample Detection\":\n",
    "                    # Apply two threshold values\n",
    "                    max_outliers_ratio = threshold_sliders[test_name][0].value\n",
    "                    outlier_score_threshold = threshold_sliders[test_name][1].value\n",
    "                    test[1].add_condition_outlier_ratio_less_or_equal(\n",
    "                        max_outliers_ratio=max_outliers_ratio,\n",
    "                        outlier_score_threshold=outlier_score_threshold\n",
    "                    )\n",
    "                elif test_name in threshold_sliders:\n",
    "                    # Apply single threshold value\n",
    "                    threshold_value = threshold_sliders[test_name].value\n",
    "                    test[1].add_condition_greater_than(threshold_value)\n",
    "            except AttributeError:\n",
    "                output.append_stdout(f\"Warning: No threshold condition for {test_name}\\n\")\n",
    "        \n",
    "        # Run the selected suite with the correct dataset format\n",
    "        if selected_suite_name == 'Data Integrity':\n",
    "            result = selected_suite.run(train_dataset)\n",
    "        elif selected_suite_name == 'Train-Test Validation':\n",
    "            result = selected_suite.run(train_dataset, test_dataset)\n",
    "        elif selected_suite_name == 'Model Evaluation':\n",
    "            result = selected_suite.run(train_dataset, test_dataset, model)\n",
    "        \n",
    "        # Ensure all tab titles are strings before displaying\n",
    "        for check_result in result.results:\n",
    "            check_result = [str(item) if isinstance(item, int) else item for item in check_result.display]\n",
    "\n",
    "        result.show()\n",
    "        \n",
    "        # -------------------- Save the Results to a Word File --------------------\n",
    "        try:\n",
    "            doc = Document()\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            \n",
    "            doc.add_heading(f'Deepchecks Report - {selected_suite_name}', level=1)\n",
    "            doc.add_paragraph(f'Run Timestamp: {timestamp}\\n')\n",
    "            \n",
    "            summary = result.to_dataframe()\n",
    "            doc.add_heading('Test Summary', level=2)\n",
    "            \n",
    "            for index, row in summary.iterrows():\n",
    "                doc.add_paragraph(f'Test: {row[\"Check\"]} | Status: {row[\"Result\"]} | Value: {row[\"Value\"]}')\n",
    "            \n",
    "            doc.add_heading('Detailed Results', level=2)\n",
    "            for check_result in result.get_not_passed_checks():\n",
    "                doc.add_paragraph(f'Check Name: {check_result.name}')\n",
    "                doc.add_paragraph(f'Description: {check_result.description}')\n",
    "                doc.add_paragraph(f'Result: {check_result.result}')\n",
    "                doc.add_paragraph(f'Message: {check_result.display()}')\n",
    "                doc.add_paragraph('---' * 10)\n",
    "            \n",
    "            file_name = f'{selected_suite_name}_Report_{timestamp}.docx'\n",
    "            doc.save(file_name)\n",
    "            output.append_stdout(f\"Results saved to {file_name}\\n\")\n",
    "        except Exception as e:\n",
    "            output.append_stdout(f\"Error saving results: {str(e)}\\n\")\n",
    "\n",
    "# Attach button click event\n",
    "run_button.on_click(run_selected_suite)\n",
    "\n",
    "# -------------------- Step 6: Display the Jupyter Widget UI --------------------\n",
    "ui = VBox([\n",
    "    suite_dropdown, \n",
    "    test_selection_box,  \n",
    "    run_button,  \n",
    "    output  \n",
    "])\n",
    "\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1afe4-83a0-4c5a-8e6b-17830ad0e6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60167d-150d-4652-9e6d-1e70d12bbb81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4986df-abe0-490e-8ffc-96a132f8ad71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd07ae39-b0a7-4ea9-be96-df29e94865d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4899f94-508a-4c53-9889-10221a6db1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0016ebd-0d2a-4c14-9e38-f818b499442a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0ce6b-46bf-4c9d-9cec-67559bf7e873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de82d2d-f44e-4eb4-a177-109d0de0953a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b511dbdd-56d1-48c5-bcbf-1699dd978451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1aa6af80c614366b53f2eb873ec74c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Suite:', options=('Data Integrity', 'Train-Test Validation', 'Model Evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### WORKING CODE FILE\n",
    "\n",
    "\n",
    "import deepchecks\n",
    "\n",
    "# Check if CheckFailure is part of the deepchecks suite (check if it's defined in your version)\n",
    "CheckFailure = deepchecks.CheckFailure\n",
    "\n",
    "# Import Required Libraries\n",
    "import deepchecks\n",
    "from deepchecks.tabular.suites import data_integrity, train_test_validation, model_evaluation\n",
    "from ipywidgets import VBox, HBox, Dropdown, FloatSlider, Button, Checkbox, Output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from docx import Document  # For saving results\n",
    "from datetime import datetime  # For unique filenames\n",
    "\n",
    "# -------------------- Step 1: Create Sample Data --------------------\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "from deepchecks.tabular import Dataset\n",
    "\n",
    "# Convert NumPy arrays to Deepchecks Dataset\n",
    "train_dataset = Dataset(pd.DataFrame(X_train), label=pd.Series(y_train))\n",
    "test_dataset = Dataset(pd.DataFrame(X_test), label=pd.Series(y_test))\n",
    "\n",
    "\n",
    "# -------------------- Step 2: Train a Simple Model --------------------\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# -------------------- Step 3: Create Jupyter Widgets --------------------\n",
    "available_suites = {\n",
    "    'Data Integrity': data_integrity(),\n",
    "    'Train-Test Validation': train_test_validation(),\n",
    "    'Model Evaluation': model_evaluation()\n",
    "}\n",
    "\n",
    "# Dropdown to select the suite\n",
    "suite_dropdown = Dropdown(\n",
    "    options=list(available_suites.keys()),\n",
    "    value='Data Integrity',\n",
    "    description='Suite:'\n",
    ")\n",
    "\n",
    "# Checkboxes and sliders for each test (Populated dynamically)\n",
    "test_checkboxes = {}\n",
    "threshold_sliders = {}\n",
    "test_selection_box = VBox([])\n",
    "\n",
    "# Button to run the suite\n",
    "run_button = Button(description=\"Run Suite\")\n",
    "\n",
    "# Output to display results\n",
    "output = Output()\n",
    "\n",
    "# -------------------- Step 4: Update Test Selection When Suite Changes --------------------\n",
    "def update_test_selection(change):\n",
    "    \"\"\"Update available tests (checkboxes and sliders) when the suite changes.\"\"\"\n",
    "    global test_checkboxes, threshold_sliders\n",
    "    selected_suite = change['new']\n",
    "    \n",
    "    # Clear old checkboxes and sliders\n",
    "    test_checkboxes.clear()\n",
    "    threshold_sliders.clear()\n",
    "    \n",
    "    test_widgets = []\n",
    "    for test_key, test_obj in available_suites[selected_suite].checks.items():\n",
    "        test_name = test_obj.name()\n",
    "        \n",
    "        # Create a checkbox for the test\n",
    "        test_checkboxes[test_name] = Checkbox(value=True, description=test_name)\n",
    "        \n",
    "        # Add threshold sliders only for relevant tests\n",
    "        slider_widget = None\n",
    "        if test_name == \"Outlier Sample Detection\":\n",
    "            # Needs two sliders for max_outliers_ratio and outlier_score_threshold\n",
    "            max_outliers_slider = FloatSlider(value=0.01, min=0.0, max=0.1, step=0.001, description='Max Outliers')\n",
    "            score_threshold_slider = FloatSlider(value=0.9, min=0.0, max=1.0, step=0.05, description='Score Threshold')\n",
    "            threshold_sliders[test_name] = (max_outliers_slider, score_threshold_slider)\n",
    "            slider_widget = HBox([max_outliers_slider, score_threshold_slider])\n",
    "        \n",
    "        elif hasattr(test_obj, \"add_condition_greater_than\"):\n",
    "            # Generic case for tests that support 'add_condition_greater_than'\n",
    "            threshold_sliders[test_name] = FloatSlider(value=0.8, min=0.0, max=1.0, step=0.05, description='Threshold')\n",
    "            slider_widget = threshold_sliders[test_name]\n",
    "\n",
    "        row_widgets = [test_checkboxes[test_name]]\n",
    "        if slider_widget:\n",
    "            row_widgets.append(slider_widget)\n",
    "        \n",
    "        test_widgets.append(HBox(row_widgets))\n",
    "    \n",
    "    test_selection_box.children = test_widgets\n",
    "\n",
    "update_test_selection({'new': 'Data Integrity'})  # Initialize UI\n",
    "suite_dropdown.observe(update_test_selection, names='value')\n",
    "\n",
    "# -------------------- Step 5: Run the Selected Suite and Save Results --------------------\n",
    "def run_selected_suite(b):\n",
    "    \"\"\"Run the selected suite with thresholds and save results to a Word file.\"\"\"\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        \n",
    "        selected_suite_name = suite_dropdown.value\n",
    "        selected_suite = available_suites[selected_suite_name]\n",
    "        \n",
    "        # Filter selected tests\n",
    "        selected_tests = [test for test in selected_suite.checks.items() if test_checkboxes[test[1].name()].value]\n",
    "        \n",
    "        # Set custom thresholds for selected tests\n",
    "        for test in selected_tests:\n",
    "            test_name = test[1].name()\n",
    "            try:\n",
    "                if test_name == \"Outlier Sample Detection\":\n",
    "                    # Apply two threshold values\n",
    "                    max_outliers_ratio = threshold_sliders[test_name][0].value\n",
    "                    outlier_score_threshold = threshold_sliders[test_name][1].value\n",
    "                    test[1].add_condition_outlier_ratio_less_or_equal(\n",
    "                        max_outliers_ratio=max_outliers_ratio,\n",
    "                        outlier_score_threshold=outlier_score_threshold\n",
    "                    )\n",
    "                elif test_name in threshold_sliders:\n",
    "                    # Apply single threshold value\n",
    "                    threshold_value = threshold_sliders[test_name].value\n",
    "                    test[1].add_condition_greater_than(threshold_value)\n",
    "            except AttributeError:\n",
    "                output.append_stdout(f\"Warning: No threshold condition for {test_name}\\n\")\n",
    "        \n",
    "        # Run the selected suite with the correct dataset format\n",
    "        if selected_suite_name == 'Data Integrity':\n",
    "            result = selected_suite.run(train_dataset)\n",
    "        elif selected_suite_name == 'Train-Test Validation':\n",
    "            result = selected_suite.run(train_dataset, test_dataset)\n",
    "        elif selected_suite_name == 'Model Evaluation':\n",
    "            result = selected_suite.run(train_dataset, test_dataset, model)\n",
    "        \n",
    "        # Now show the results after processing\n",
    "        #result.show()\n",
    "        result.save_as_html('my_results.html')\n",
    "        \n",
    "        # -------------------- Save the Results to a Word File --------------------\n",
    "        try:\n",
    "            doc = Document()\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            \n",
    "            doc.add_heading(f'Deepchecks Report - {selected_suite_name}', level=1)\n",
    "            doc.add_paragraph(f'Run Timestamp: {timestamp}\\n')\n",
    "            \n",
    "            summary = result.to_dataframe()\n",
    "            doc.add_heading('Test Summary', level=2)\n",
    "            \n",
    "            for index, row in summary.iterrows():\n",
    "                doc.add_paragraph(f'Test: {row[\"Check\"]} | Status: {row[\"Result\"]} | Value: {row[\"Value\"]}')\n",
    "            \n",
    "            doc.add_heading('Detailed Results', level=2)\n",
    "            for check_result in result.get_not_passed_checks():\n",
    "                doc.add_paragraph(f'Check Name: {check_result.name}')\n",
    "                doc.add_paragraph(f'Description: {check_result.description}')\n",
    "                doc.add_paragraph(f'Result: {check_result.result}')\n",
    "                doc.add_paragraph(f'Message: {check_result.display()}')\n",
    "                doc.add_paragraph('---' * 10)\n",
    "            \n",
    "            file_name = f'{selected_suite_name}_Report_{timestamp}.docx'\n",
    "            print(file_name)\n",
    "            doc.save(file_name)\n",
    "            output.append_stdout(f\"Results saved to {file_name}\\n\")\n",
    "        except Exception as e:\n",
    "            output.append_stdout(f\"Error saving results: {str(e)}\\n\")\n",
    "\n",
    "# Attach button click event\n",
    "run_button.on_click(run_selected_suite)\n",
    "\n",
    "# -------------------- Step 6: Display the Jupyter Widget UI --------------------\n",
    "ui = VBox([\n",
    "    suite_dropdown, \n",
    "    test_selection_box,  \n",
    "    run_button,  \n",
    "    output  \n",
    "])\n",
    "\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e555e18c-f253-4419-a3e2-1db1b63e3058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3819d9-68c3-4521-ba13-2186c292fa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc367371-8b8f-43b5-a16c-b133976928ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge deepchecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e141d542-ef55-4932-b273-f0b4c9d58d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install deepchecks --quiet --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2dd3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchecks\n",
    "from deepchecks.tabular.suites import data_integrity\n",
    "\n",
    "#from deepchecks.tabular.suites import data_integrity, train_test_validation, model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f689e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b983b533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deepchecks\\suites.py:21: DeprecationWarning:\n",
      "\n",
      "Ability to import tabular suites from the `deepchecks.suites` is deprecated, please import from `deepchecks.tabular.suites` instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24bb57323094f6285cc4262ddaefed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Suite:', options=('Data Integrity', 'Train-Test Validation', 'Model Evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Import Required Libraries\n",
    "import deepchecks\n",
    "from deepchecks.suites import data_integrity, train_test_validation, model_evaluation\n",
    "from ipywidgets import interactive, VBox, HBox, Dropdown, FloatSlider, Button, Checkbox, Output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from docx import Document  # For saving the results to a Word document\n",
    "from datetime import datetime  # To generate unique filenames\n",
    "\n",
    "# -------------------- Step 1: Create Sample Data (Optional) --------------------\n",
    "# If you have your own data, you can skip this step and load your dataset instead.\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------- Step 2: Train a Simple Model (Optional) --------------------\n",
    "# Train a RandomForest model as an example\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# -------------------- Step 3: Create Jupyter Widgets --------------------\n",
    "# Available deepchecks suites\n",
    "available_suites = {\n",
    "    'Data Integrity': data_integrity(),\n",
    "    'Train-Test Validation': train_test_validation(),\n",
    "    'Model Evaluation': model_evaluation()\n",
    "}\n",
    "\n",
    "\n",
    "# Dropdown widget to select the suite\n",
    "suite_dropdown = Dropdown(\n",
    "    options=list(available_suites.keys()),\n",
    "    value='Data Integrity',\n",
    "    description='Suite:'\n",
    ")\n",
    "\n",
    "# Checkboxes to select which tests to run within the suite\n",
    "test_checkboxes = {test[0]: Checkbox(value=True, description=test[1].name()) \n",
    "                   for test in available_suites['Data Integrity'].checks.items()}\n",
    "test_selection_box = VBox(list(test_checkboxes.values()))\n",
    "\n",
    "# Slider to set the threshold for passing the test\n",
    "threshold_slider = FloatSlider(\n",
    "    value=0.8, \n",
    "    min=0.0, \n",
    "    max=1.0, \n",
    "    step=0.05, \n",
    "    description='Threshold:'\n",
    ")\n",
    "\n",
    "# Button to run the suite\n",
    "run_button = Button(description=\"Run Suite\")\n",
    "\n",
    "# Output area to display the suite results\n",
    "output = Output()\n",
    "\n",
    "# -------------------- Step 4: Update Test Selection When Suite Changes --------------------\n",
    "def update_test_selection(change):\n",
    "    \"\"\"Update the available tests when the suite changes.\"\"\"\n",
    "    global test_checkboxes\n",
    "    selected_suite = change['new']\n",
    "    test_checkboxes = {test[0]: Checkbox(value=True, description=test[1].name()) \n",
    "                       for test in available_suites[selected_suite].checks.items()}\n",
    "    test_selection_box.children = list(test_checkboxes.values())\n",
    "\n",
    "suite_dropdown.observe(update_test_selection, names='value')\n",
    "\n",
    "# -------------------- Step 5: Run the Selected Suite and Save Results --------------------\n",
    "def run_selected_suite(b):\n",
    "    \"\"\"Run the selected suite with selected tests and thresholds, then save the results to a Word file.\"\"\"\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        selected_suite_name = suite_dropdown.value\n",
    "        selected_suite = available_suites[selected_suite_name]\n",
    "        print(selected_suite)\n",
    "        \n",
    "        #for test in selected_suite.checks:\n",
    "            #print(test[1])\n",
    "            #print(test)\n",
    "            #print(test_checkboxes[0].value)\n",
    "        # Filter the selected tests\n",
    "        selected_tests = [test for i, test in selected_suite.checks.items() if test_checkboxes[i].value]\n",
    "        print(\"selected_tests\")\n",
    "        print(selected_tests)\n",
    "        # Set a threshold for each test\n",
    "        for test in selected_tests:\n",
    "            print(test)\n",
    "            test.add_condition_test_score_greater_than(threshold_slider.value)\n",
    "        \n",
    "        # Run the suite and display the results\n",
    "        if selected_suite_name == 'Data Integrity':\n",
    "            result = selected_suite.run(X_train)\n",
    "        elif selected_suite_name == 'Train-Test Validation':\n",
    "            result = selected_suite.run(X_train, X_test, y_train, y_test)\n",
    "        elif selected_suite_name == 'Model Evaluation':\n",
    "            result = selected_suite.run(X_train, y_train, X_test, y_test, model)\n",
    "        \n",
    "        result.show()\n",
    "        \n",
    "        # -------------------- Save the Results to a Word File --------------------\n",
    "        try:\n",
    "            # Create a new Word document\n",
    "            doc = Document()\n",
    "            \n",
    "            # Add a title with the suite name and timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            doc.add_heading(f'Deepchecks Report - {selected_suite_name}', level=1)\n",
    "            doc.add_paragraph(f'Run Timestamp: {timestamp}\\n')\n",
    "            \n",
    "            # Add a summary of the suite results\n",
    "            summary = result.to_dataframe()\n",
    "            doc.add_heading('Test Summary', level=2)\n",
    "            \n",
    "            for index, row in summary.iterrows():\n",
    "                doc.add_paragraph(f'Test: {row[\"Check\"]} | Status: {row[\"Result\"]} | Value: {row[\"Value\"]}')\n",
    "            \n",
    "            # Add detailed results for each test\n",
    "            doc.add_heading('Detailed Results', level=2)\n",
    "            for check_result in result.get_not_passed_checks():\n",
    "                doc.add_paragraph(f'Check Name: {check_result.name}')\n",
    "                doc.add_paragraph(f'Description: {check_result.description}')\n",
    "                doc.add_paragraph(f'Result: {check_result.result}')\n",
    "                doc.add_paragraph(f'Message: {check_result.display()}')\n",
    "                doc.add_paragraph('---' * 10)  # Divider\n",
    "            \n",
    "            # Save the Word document\n",
    "            file_name = f'{selected_suite_name}_Report_{timestamp}.docx'\n",
    "            doc.save(file_name)\n",
    "            \n",
    "            output.append_stdout(f\"Results saved to {file_name}\\n\")\n",
    "        except Exception as e:\n",
    "            output.append_stdout(f\"Error saving results: {str(e)}\\n\")\n",
    "\n",
    "# Attach the \"run suite\" button to the handler function\n",
    "run_button.on_click(run_selected_suite)\n",
    "\n",
    "# -------------------- Step 6: Display the Jupyter Widget UI --------------------\n",
    "# Layout for the widget\n",
    "ui = VBox([\n",
    "    suite_dropdown,  # Dropdown to select the suite\n",
    "    test_selection_box,  # Checkboxes to select which tests to run\n",
    "    threshold_slider,  # Slider to set test pass threshold\n",
    "    run_button,  # Button to run the suite\n",
    "    output  # Output display for test results and file save status\n",
    "])\n",
    "\n",
    "# Display the interactive UI\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af693f0-380d-4b9d-9a5a-79390324c1f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "DeepchecksValueError",
     "evalue": "At least one dataset (or model) must be passed to the method!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDeepchecksValueError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepchecks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabular\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msuites\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_integrity\n\u001b[0;32m      2\u001b[0m suite \u001b[38;5;241m=\u001b[39m data_integrity(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m], n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000_000\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msuite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deepchecks\\tabular\\suite.py:76\u001b[0m, in \u001b[0;36mSuite.run\u001b[1;34m(self, train_dataset, test_dataset, model, feature_importance, feature_importance_force_permutation, feature_importance_timeout, with_display, y_pred_train, y_pred_test, y_proba_train, y_proba_test, run_single_dataset, model_classes)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@docstrings\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     model_classes: Optional[List] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     56\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SuiteResult:\n\u001b[0;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run all checks.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m        All results by all initialized checks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mContext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_importance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_importance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_importance_force_permutation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_importance_force_permutation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_importance_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_importance_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_display\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_display\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_proba_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_proba_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_proba_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_proba_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     progress_bar \u001b[38;5;241m=\u001b[39m create_progress_bar(\n\u001b[0;32m     92\u001b[0m         iterable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecks\u001b[38;5;241m.\u001b[39mvalues()),\n\u001b[0;32m     93\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m     94\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCheck\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m     )\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# Run all checks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deepchecks\\tabular\\context.py:192\u001b[0m, in \u001b[0;36mContext.__init__\u001b[1;34m(self, train, test, model, feature_importance, feature_importance_force_permutation, feature_importance_timeout, with_display, y_pred_train, y_pred_test, y_proba_train, y_proba_test, model_classes)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    177\u001b[0m         train: t\u001b[38;5;241m.\u001b[39mUnion[Dataset, pd\u001b[38;5;241m.\u001b[39mDataFrame, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m ):\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# Validations\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DeepchecksValueError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAt least one dataset (or model) must be passed to the method!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m         train \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mcast_to_dataset(train)\n",
      "\u001b[1;31mDeepchecksValueError\u001b[0m: At least one dataset (or model) must be passed to the method!"
     ]
    }
   ],
   "source": [
    "from deepchecks.tabular.suites import data_integrity\n",
    "suite = data_integrity(columns=['a', 'b', 'c'], n_samples=1_000_000)\n",
    "result = suite.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ead07afa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3432277579.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(j.)\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for i,j in data_integrity().checks.items():\n",
    "    print(j.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ca335eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'j' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m((\u001b[43mj\u001b[49m\u001b[38;5;241m.\u001b[39mparams()\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m j\u001b[38;5;241m.\u001b[39mparams()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'j' is not defined"
     ]
    }
   ],
   "source": [
    "print((j.params().keys())[0])\n",
    "j.params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caea3807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None IsSingleValue\n",
      "1 None SpecialCharacters\n",
      "2 None MixedNulls\n",
      "3 None MixedDataTypes\n",
      "4 None StringMismatch\n",
      "5 None DataDuplicates\n",
      "6 None StringLengthOutOfBounds\n",
      "7 None ConflictingLabels\n",
      "8 None OutlierSampleDetection\n",
      "9 None FeatureLabelCorrelation(ppscore_params={}, random_state=42)\n",
      "10 None FeatureFeatureCorrelation\n",
      "11 None IdentifierLabelCorrelation(ppscore_params={})\n"
     ]
    }
   ],
   "source": [
    "for i,j in data_integrity().checks.items():\n",
    "    print(i, j.clean_conditions(), j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee570ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9efbf769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc21ea1707144deac30c0df782b0ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=True, description='Is Single Value'), Checkbox(value=True, description='Special …"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_selection_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86f41c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slider to set the threshold for passing the test\n",
    "threshold_slider = FloatSlider(\n",
    "    value=0.8, \n",
    "    min=0.0, \n",
    "    max=1.0, \n",
    "    step=0.05, \n",
    "    description='Threshold:'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "679c8397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6ff457f1444f11b42baa6611bae172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.8, description='Threshold:', max=1.0, step=0.05)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bafe9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0a9b636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7089865bf15644f7b34752e470033de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Suite', style=ButtonStyle())"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "896d863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import deepchecks\n",
    "from deepchecks.tabular.suites import data_integrity, train_test_validation, model_evaluation\n",
    "\n",
    "# Available deepchecks suites\n",
    "available_suites = {\n",
    "    'Data Integrity': data_integrity(),\n",
    "    'Train-Test Validation': train_test_validation(),\n",
    "    'Model Evaluation': model_evaluation()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f042743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Single Value\n",
      "{'class_name': 'IsSingleValue', 'module_name': 'deepchecks.tabular.checks.data_integrity.is_single_value', 'params': {'columns': None, 'ignore_columns': None, 'ignore_nan': True, 'n_samples': 10000000, 'random_state': 42}, 'version': '0.19.0'}\n",
      "{'name': 'Is Single Value', 'params': {'columns': None, 'ignore_columns': None, 'ignore_nan': True, 'n_samples': 10000000, 'random_state': 42}, 'summary': 'Check if there are columns which have only a single unique value in all rows.'}\n",
      "Special Characters\n",
      "{'class_name': 'SpecialCharacters', 'module_name': 'deepchecks.tabular.checks.data_integrity.special_chars', 'params': {'columns': None, 'ignore_columns': None, 'n_most_common': 2, 'n_top_columns': 10, 'n_samples': 10000000, 'random_state': 42}, 'version': '0.19.0'}\n",
      "{'name': 'Special Characters', 'params': {'columns': None, 'ignore_columns': None, 'n_most_common': 2, 'n_top_columns': 10, 'n_samples': 10000000, 'random_state': 42}, 'summary': 'Search in column[s] for values that contains only special characters.'}\n",
      "Mixed Nulls\n",
      "{'class_name': 'MixedNulls', 'module_name': 'deepchecks.tabular.checks.data_integrity.mixed_nulls', 'params': {'null_string_list': None, 'check_nan': True, 'columns': None, 'ignore_columns': None, 'n_top_columns': 10, 'aggregation_method': 'max', 'n_samples': 10000000, 'random_state': 42}, 'version': '0.19.0'}\n",
      "{'name': 'Mixed Nulls', 'params': {'null_string_list': None, 'check_nan': True, 'columns': None, 'ignore_columns': None, 'n_top_columns': 10, 'aggregation_method': 'max', 'n_samples': 10000000, 'random_state': 42}, 'summary': 'Search for various types of null values, including string representations of null.'}\n",
      "Mixed Data Types\n",
      "{'class_name': 'MixedDataTypes', 'module_name': 'deepchecks.tabular.checks.data_integrity.mixed_data_types', 'params': {'columns': None, 'ignore_columns': None, 'n_top_columns': 10, 'n_samples': 10000000, 'random_state': 42}, 'version': '0.19.0'}\n",
      "{'name': 'Mixed Data Types', 'params': {'columns': None, 'ignore_columns': None, 'n_top_columns': 10, 'n_samples': 10000000, 'random_state': 42}, 'summary': 'Detect columns which contain a mix of numerical and string values.'}\n",
      "String Mismatch\n",
      "{'class_name': 'StringMismatch', 'module_name': 'deepchecks.tabular.checks.data_integrity.string_mismatch', 'params': {'columns': None, 'ignore_columns': None, 'n_top_columns': 10, 'aggregation_method': 'max', 'n_samples': 1000000, 'random_state': 42}, 'version': '0.19.0'}\n",
      "{'name': 'String Mismatch', 'params': {'columns': None, 'ignore_columns': None, 'n_top_columns': 10, 'aggregation_method': 'max', 'n_samples': 1000000, 'random_state': 42}, 'summary': 'Detect different variants of string categories (e.g. \"mislabeled\" vs \"mis-labeled\") in a categorical column.'}\n",
      "Data Duplicates\n",
      "{'class_name': 'DataDuplicates', 'module_name': 'deepchecks.tabular.checks.data_integrity.data_duplicates', 'params': {'columns': None, 'ignore_columns': None, 'n_to_show': 5, 'n_samples': 10000000, 'random_state': 42}, 'version': '0.19.0'}\n",
      "{'name': 'Data Duplicates', 'params': {'columns': None, 'ignore_columns': None, 'n_to_show': 5, 'n_samples': 10000000, 'random_state': 42}, 'summary': 'Checks for duplicate samples in the dataset.'}\n",
      "String Length Out Of Bounds\n",
      "{'class_name': 'StringLengthOutOfBounds', 'module_name': 'deepchecks.tabular.checks.data_integrity.string_length_out_of_bounds', 'params': {'columns': None, 'ignore_columns': None, 'num_percentiles': 1000, 'inner_quantile_range': 94, 'outlier_factor': 4, 'min_length_difference': 5, 'min_length_ratio_difference': 0.5, 'min_unique_value_ratio': 0.01, 'min_unique_values': 100, 'n_top_columns': 10, 'outlier_length_to_show': 50, 'samples_per_range_to_show': 3, 'n_samples': 10000000, 'random_state': 42}, 'version': '0.19.0'}\n",
      "{'name': 'String Length Out Of Bounds', 'params': {'columns': None, 'ignore_columns': None, 'num_percentiles': 1000, 'inner_quantile_range': 94, 'outlier_factor': 4, 'min_length_difference': 5, 'min_length_ratio_difference': 0.5, 'min_unique_value_ratio': 0.01, 'min_unique_values': 100, 'n_top_columns': 10, 'outlier_length_to_show': 50, 'samples_per_range_to_show': 3, 'n_samples': 10000000, 'random_state': 42}, 'summary': 'Detect strings with length that is much longer/shorter than the identified \"normal\" string lengths.'}\n",
      "Conflicting Labels\n",
      "{'class_name': 'ConflictingLabels', 'module_name': 'deepchecks.tabular.checks.data_integrity.conflicting_labels', 'params': {'columns': None, 'ignore_columns': None, 'n_to_show': 5, 'n_samples': 10000000, 'random_state': 42}, 'version': '0.19.0'}\n",
      "{'name': 'Conflicting Labels', 'params': {'columns': None, 'ignore_columns': None, 'n_to_show': 5, 'n_samples': 10000000, 'random_state': 42}, 'summary': \"Find samples which have the exact same features' values but different labels.\"}\n",
      "Outlier Sample Detection\n",
      "{'class_name': 'OutlierSampleDetection', 'module_name': 'deepchecks.tabular.checks.data_integrity.outlier_sample_detection', 'params': {'columns': None, 'ignore_columns': None, 'nearest_neighbors_percent': 0.01, 'extent_parameter': 3, 'n_samples': 5000, 'n_to_show': 5, 'random_state': 42, 'timeout': 10}, 'version': '0.19.0'}\n",
      "{'name': 'Outlier Sample Detection', 'params': {'columns': None, 'ignore_columns': None, 'nearest_neighbors_percent': 0.01, 'extent_parameter': 3, 'n_samples': 5000, 'n_to_show': 5, 'random_state': 42, 'timeout': 10}, 'summary': 'Detects outliers in a dataset using the LoOP algorithm.'}\n",
      "Feature Label Correlation\n",
      "{'class_name': 'FeatureLabelCorrelation', 'module_name': 'deepchecks.tabular.checks.data_integrity.feature_label_correlation', 'params': {'ppscore_params': {}, 'n_top_features': 5, 'n_samples': 100000, 'random_state': 42}, 'version': '0.19.0'}\n",
      "{'name': 'Feature Label Correlation', 'params': {'ppscore_params': {}, 'n_top_features': 5, 'n_samples': 100000, 'random_state': 42}, 'summary': 'Return the PPS (Predictive Power Score) of all features in relation to the label.'}\n",
      "Feature Feature Correlation\n",
      "{'class_name': 'FeatureFeatureCorrelation', 'module_name': 'deepchecks.tabular.checks.data_integrity.feature_feature_correlation', 'params': {'columns': None, 'ignore_columns': None, 'show_n_top_columns': 10, 'n_samples': 10000, 'random_state': 42}, 'version': '0.19.0'}\n",
      "{'name': 'Feature Feature Correlation', 'params': {'columns': None, 'ignore_columns': None, 'show_n_top_columns': 10, 'n_samples': 10000, 'random_state': 42}, 'summary': '    Checks for pairwise correlation between the features.'}\n",
      "Identifier Label Correlation\n",
      "{'class_name': 'IdentifierLabelCorrelation', 'module_name': 'deepchecks.tabular.checks.data_integrity.identifier_label_correlation', 'params': {'ppscore_params': {}, 'n_samples': 1000000, 'random_state': 42}, 'version': '0.19.0'}\n",
      "{'name': 'Identifier Label Correlation', 'params': {'ppscore_params': {}, 'n_samples': 1000000, 'random_state': 42}, 'summary': 'Check if identifiers (Index/Date) can be used to predict the label.'}\n"
     ]
    }
   ],
   "source": [
    "for test in available_suites['Data Integrity'].checks.items():\n",
    "    #print(test)\n",
    "    print(test[1].name())\n",
    "    #print()\n",
    "    print(test[1].config())\n",
    "    print(test[1].metadata())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f5193ec-b53e-4010-98d8-848befc57d3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conditions_decision() missing 1 required positional argument: 'result'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconditions_decision\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: conditions_decision() missing 1 required positional argument: 'result'"
     ]
    }
   ],
   "source": [
    "test[1].conditions_decision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23a02fc0-457e-42b2-96c2-d5996c220a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ppscore_params': {}}\n",
      "Identifier Label Correlation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'Identifier Label Correlation',\n",
       " 'params': {'ppscore_params': {0.1}, 'n_samples': 1000000, 'random_state': 42},\n",
       " 'summary': 'Check if identifiers (Index/Date) can be used to predict the label.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(test[1].metadata())\n",
    "\n",
    "print(test[1].params())\n",
    "print(test[1].name())\n",
    "#test[1].remove_condition(0)\n",
    "\n",
    "from deepchecks.tabular.checks import IdentifierLabelCorrelation\n",
    "\n",
    "test[1].ppscore_params={0.1}\n",
    "test[1].metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6d5157b-57bb-4c34-9351-54ef868eaf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111---\n",
      "IsSingleValue\n",
      "\tConditions:\n",
      "\t\t0: Does not contain only a single value\n",
      "2222---\n",
      "{'name': 'Is Single Value', 'params': {'columns': None, 'ignore_columns': None, 'ignore_nan': True, 'n_samples': 10000000, 'random_state': 42}, 'summary': 'Check if there are columns which have only a single unique value in all rows.'}\n",
      "3333---\n",
      "Is Single Value\n",
      "1111---\n",
      "SpecialCharacters\n",
      "\tConditions:\n",
      "\t\t0: Ratio of samples containing solely special character is less or equal to 0.1%\n",
      "2222---\n",
      "{'name': 'Special Characters', 'params': {'columns': None, 'ignore_columns': None, 'n_most_common': 2, 'n_top_columns': 10, 'n_samples': 10000000, 'random_state': 42}, 'summary': 'Search in column[s] for values that contains only special characters.'}\n",
      "3333---\n",
      "Special Characters\n",
      "1111---\n",
      "MixedNulls\n",
      "\tConditions:\n",
      "\t\t0: Number of different null types is less or equal to 1\n",
      "2222---\n",
      "{'name': 'Mixed Nulls', 'params': {'null_string_list': None, 'check_nan': True, 'columns': None, 'ignore_columns': None, 'n_top_columns': 10, 'aggregation_method': 'max', 'n_samples': 10000000, 'random_state': 42}, 'summary': 'Search for various types of null values, including string representations of null.'}\n",
      "3333---\n",
      "Mixed Nulls\n",
      "1111---\n",
      "MixedDataTypes\n",
      "\tConditions:\n",
      "\t\t0: Rare data types in column are either more than 10% or less than 1% of the data\n",
      "2222---\n",
      "{'name': 'Mixed Data Types', 'params': {'columns': None, 'ignore_columns': None, 'n_top_columns': 10, 'n_samples': 10000000, 'random_state': 42}, 'summary': 'Detect columns which contain a mix of numerical and string values.'}\n",
      "3333---\n",
      "Mixed Data Types\n",
      "1111---\n",
      "StringMismatch\n",
      "\tConditions:\n",
      "\t\t0: No string variants\n",
      "2222---\n",
      "{'name': 'String Mismatch', 'params': {'columns': None, 'ignore_columns': None, 'n_top_columns': 10, 'aggregation_method': 'max', 'n_samples': 1000000, 'random_state': 42}, 'summary': 'Detect different variants of string categories (e.g. \"mislabeled\" vs \"mis-labeled\") in a categorical column.'}\n",
      "3333---\n",
      "String Mismatch\n",
      "1111---\n",
      "DataDuplicates\n",
      "\tConditions:\n",
      "\t\t0: Duplicate data ratio is less or equal to 5%\n",
      "2222---\n",
      "{'name': 'Data Duplicates', 'params': {'columns': None, 'ignore_columns': None, 'n_to_show': 5, 'n_samples': 10000000, 'random_state': 42}, 'summary': 'Checks for duplicate samples in the dataset.'}\n",
      "3333---\n",
      "Data Duplicates\n",
      "1111---\n",
      "StringLengthOutOfBounds\n",
      "\tConditions:\n",
      "\t\t0: Ratio of string length outliers is less or equal to 0%\n",
      "2222---\n",
      "{'name': 'String Length Out Of Bounds', 'params': {'columns': None, 'ignore_columns': None, 'num_percentiles': 1000, 'inner_quantile_range': 94, 'outlier_factor': 4, 'min_length_difference': 5, 'min_length_ratio_difference': 0.5, 'min_unique_value_ratio': 0.01, 'min_unique_values': 100, 'n_top_columns': 10, 'outlier_length_to_show': 50, 'samples_per_range_to_show': 3, 'n_samples': 10000000, 'random_state': 42}, 'summary': 'Detect strings with length that is much longer/shorter than the identified \"normal\" string lengths.'}\n",
      "3333---\n",
      "String Length Out Of Bounds\n",
      "1111---\n",
      "ConflictingLabels\n",
      "\tConditions:\n",
      "\t\t0: Ambiguous sample ratio is less or equal to 0%\n",
      "2222---\n",
      "{'name': 'Conflicting Labels', 'params': {'columns': None, 'ignore_columns': None, 'n_to_show': 5, 'n_samples': 10000000, 'random_state': 42}, 'summary': \"Find samples which have the exact same features' values but different labels.\"}\n",
      "3333---\n",
      "Conflicting Labels\n",
      "1111---\n",
      "OutlierSampleDetection\n",
      "2222---\n",
      "{'name': 'Outlier Sample Detection', 'params': {'columns': None, 'ignore_columns': None, 'nearest_neighbors_percent': 0.01, 'extent_parameter': 3, 'n_samples': 5000, 'n_to_show': 5, 'random_state': 42, 'timeout': 10}, 'summary': 'Detects outliers in a dataset using the LoOP algorithm.'}\n",
      "3333---\n",
      "Outlier Sample Detection\n",
      "1111---\n",
      "FeatureLabelCorrelation(ppscore_params={}, random_state=42)\n",
      "\tConditions:\n",
      "\t\t0: Features' Predictive Power Score is less than 0.8\n",
      "2222---\n",
      "{'name': 'Feature Label Correlation', 'params': {'ppscore_params': {}, 'n_top_features': 5, 'n_samples': 100000, 'random_state': 42}, 'summary': 'Return the PPS (Predictive Power Score) of all features in relation to the label.'}\n",
      "3333---\n",
      "Feature Label Correlation\n",
      "1111---\n",
      "FeatureFeatureCorrelation\n",
      "\tConditions:\n",
      "\t\t0: Not more than 0 pairs are correlated above 0.9\n",
      "2222---\n",
      "{'name': 'Feature Feature Correlation', 'params': {'columns': None, 'ignore_columns': None, 'show_n_top_columns': 10, 'n_samples': 10000, 'random_state': 42}, 'summary': '    Checks for pairwise correlation between the features.'}\n",
      "3333---\n",
      "Feature Feature Correlation\n",
      "1111---\n",
      "IdentifierLabelCorrelation(ppscore_params={})\n",
      "\tConditions:\n",
      "\t\t0: Identifier columns PPS is less or equal to 0\n",
      "2222---\n",
      "{'name': 'Identifier Label Correlation', 'params': {'ppscore_params': {}, 'n_samples': 1000000, 'random_state': 42}, 'summary': 'Check if identifiers (Index/Date) can be used to predict the label.'}\n",
      "3333---\n",
      "Identifier Label Correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deepchecks\\checks.py:21: DeprecationWarning:\n",
      "\n",
      "Ability to import tabular checks from the `deepchecks.checks` is deprecated, please import from `deepchecks.tabular.checks` instead\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepchecks.tabular.suites import data_integrity\n",
    "from deepchecks.checks import MixedNulls  # Ensure this import is correct\n",
    "\n",
    "suite = data_integrity()\n",
    "\n",
    "# Step 2: Print check types to confirm the type of each check\n",
    "for check in suite.checks.items():\n",
    "    #print(check[1])\n",
    "    print(\"1111---\")\n",
    "    print(check[1])\n",
    "    print(\"2222---\")\n",
    "    print(check[1].metadata())\n",
    "    print(\"3333---\")\n",
    "    print(check[1].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6231326-8e63-4fef-a1a8-337928e33974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111---\n",
      "IdentifierLabelCorrelation(ppscore_params={})\n",
      "\tConditions:\n",
      "\t\t0: Identifier columns PPS is less or equal to 0\n",
      "2222---\n",
      "{'name': 'Identifier Label Correlation', 'params': {'ppscore_params': {}, 'n_samples': 1000000, 'random_state': 42}, 'summary': 'Check if identifiers (Index/Date) can be used to predict the label.'}\n",
      "3333---\n",
      "Identifier Label Correlation\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'odict_values' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3333---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(check[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n\u001b[1;32m----> 8\u001b[0m \u001b[43msuite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchecks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'odict_values' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(\"1111---\")\n",
    "print(check[1])\n",
    "print(\"2222---\")\n",
    "print(check[1].metadata())\n",
    "print(\"3333---\")\n",
    "print(check[1].name())\n",
    "\n",
    "suite.checks.values()[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f53c57e0-a1db-4d6a-bf01-2b1cb4bdb62d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1490723542.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[24], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(check[1].)\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from deepchecks.tabular.suites import data_integrity\n",
    "from deepchecks.checks import MixedNulls  # Ensure this import is correct\n",
    "\n",
    "suite = data_integrity()\n",
    "\n",
    "# Step 2: Print check types to confirm the type of each check\n",
    "for check in suite.checks.items():\n",
    "    print(check[1].)\n",
    "\n",
    "# Step 3: Access the MixedNulls check correctly\n",
    "for check in suite.checks:\n",
    "    if isinstance(check, MixedNulls):  # Ensure the name of the class is correct\n",
    "        mixed_nulls_check = check\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(\"MixedNulls check not found in the Data Integrity suite.\")\n",
    "\n",
    "# Step 4: Update the condition\n",
    "mixed_nulls_check.conditions = {}  # Clears all existing conditions\n",
    "mixed_nulls_check.add_condition('proportion of mixed nulls is less than', threshold=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "651c8114-2387-4208-809d-a0fb8a1d49a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1969544045.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[25], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(check.)\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(check.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e1c2b1e-e021-439b-9046-7c8eb8dcfae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IsSingleValue\n",
      "\tConditions:\n",
      "\t\t0: Does not contain only a single value\n",
      "['__abstractmethods__', '__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_conditions', '_conditions_index', '_prepare_config', 'add_condition', 'add_condition_not_single_value', 'clean_conditions', 'columns', 'conditions_decision', 'config', 'context_type', 'from_config', 'from_json', 'ignore_columns', 'ignore_nan', 'metadata', 'n_samples', 'name', 'params', 'random_state', 'remove_condition', 'run', 'run_logic', 'to_json']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'class_name': 'IsSingleValue',\n",
       " 'module_name': 'deepchecks.tabular.checks.data_integrity.is_single_value',\n",
       " 'params': {'columns': None,\n",
       "  'ignore_columns': None,\n",
       "  'ignore_nan': True,\n",
       "  'n_samples': 10000000,\n",
       "  'random_state': 42},\n",
       " 'version': '0.19.0'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepchecks.tabular.suites import data_integrity\n",
    "from deepchecks.checks import MixedNulls  # Ensure this import is correct\n",
    "\n",
    "# Step 1: Create the Data Integrity Suite\n",
    "suite = data_integrity()\n",
    "\n",
    "# Step 2: Convert the odict_values to a list\n",
    "checks_list = list(suite.checks.items())  # Convert to list to make it subscriptable\n",
    "#print(checks_list)\n",
    "print(checks_list[0][1])\n",
    "print(dir(checks_list[0][1]))\n",
    "\n",
    "checks_list[0][1].config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd4a1613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039a851fb13a4440bd3df84b3e5a018c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Suite:', options=('Data Integrity', 'Train-Test Validation', 'Model Evalu…"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output area to display the suite results\n",
    "output = Output()\n",
    "\n",
    "# -------------------- Step 4: Update Test Selection When Suite Changes --------------------\n",
    "def update_test_selection(change):\n",
    "    \"\"\"Update the available tests when the suite changes.\"\"\"\n",
    "    global test_checkboxes\n",
    "    selected_suite = change['new']\n",
    "    test_checkboxes = {test[0]: Checkbox(value=True, description=test[1].name()) \n",
    "                       for test in available_suites[selected_suite].checks.items()}\n",
    "    test_selection_box.children = list(test_checkboxes.values())\n",
    "\n",
    "suite_dropdown.observe(update_test_selection, names='value')\n",
    "\n",
    "# -------------------- Step 5: Run the Selected Suite and Save Results --------------------\n",
    "def run_selected_suite(b):\n",
    "    \"\"\"Run the selected suite with selected tests and thresholds, then save the results to a Word file.\"\"\"\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        selected_suite_name = suite_dropdown.value\n",
    "        selected_suite = available_suites[selected_suite_name]\n",
    "        \n",
    "        # Filter the selected tests\n",
    "        selected_tests = [test for test in selected_suite.checks if test_checkboxes[test.name].value]\n",
    "        \n",
    "        # Set a threshold for each test\n",
    "        for test in selected_tests:\n",
    "            test.add_condition_test_score_greater_than(threshold_slider.value)\n",
    "        \n",
    "        # Run the suite and display the results\n",
    "        if selected_suite_name == 'Data Integrity':\n",
    "            result = selected_suite.run(X_train)\n",
    "        elif selected_suite_name == 'Train-Test Validation':\n",
    "            result = selected_suite.run(X_train, X_test, y_train, y_test)\n",
    "        elif selected_suite_name == 'Model Evaluation':\n",
    "            result = selected_suite.run(X_train, y_train, X_test, y_test, model)\n",
    "        \n",
    "        result.show()\n",
    "        \n",
    "        # -------------------- Save the Results to a Word File --------------------\n",
    "        try:\n",
    "            # Create a new Word document\n",
    "            doc = Document()\n",
    "            \n",
    "            # Add a title with the suite name and timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            doc.add_heading(f'Deepchecks Report - {selected_suite_name}', level=1)\n",
    "            doc.add_paragraph(f'Run Timestamp: {timestamp}\\n')\n",
    "            \n",
    "            # Add a summary of the suite results\n",
    "            summary = result.to_dataframe()\n",
    "            doc.add_heading('Test Summary', level=2)\n",
    "            \n",
    "            for index, row in summary.iterrows():\n",
    "                doc.add_paragraph(f'Test: {row[\"Check\"]} | Status: {row[\"Result\"]} | Value: {row[\"Value\"]}')\n",
    "            \n",
    "            # Add detailed results for each test\n",
    "            doc.add_heading('Detailed Results', level=2)\n",
    "            for check_result in result.get_not_passed_checks():\n",
    "                doc.add_paragraph(f'Check Name: {check_result.name}')\n",
    "                doc.add_paragraph(f'Description: {check_result.description}')\n",
    "                doc.add_paragraph(f'Result: {check_result.result}')\n",
    "                doc.add_paragraph(f'Message: {check_result.display()}')\n",
    "                doc.add_paragraph('---' * 10)  # Divider\n",
    "            \n",
    "            # Save the Word document\n",
    "            file_name = f'{selected_suite_name}_Report_{timestamp}.docx'\n",
    "            doc.save(file_name)\n",
    "            \n",
    "            output.append_stdout(f\"Results saved to {file_name}\\n\")\n",
    "        except Exception as e:\n",
    "            output.append_stdout(f\"Error saving results: {str(e)}\\n\")\n",
    "\n",
    "# Attach the \"run suite\" button to the handler function\n",
    "run_button.on_click(run_selected_suite)\n",
    "\n",
    "# -------------------- Step 6: Display the Jupyter Widget UI --------------------\n",
    "# Layout for the widget\n",
    "ui = VBox([\n",
    "    suite_dropdown,  # Dropdown to select the suite\n",
    "    test_selection_box,  # Checkboxes to select which tests to run\n",
    "    threshold_slider,  # Slider to set test pass threshold\n",
    "    run_button,  # Button to run the suite\n",
    "    output  # Output display for test results and file save status\n",
    "])\n",
    "\n",
    "# Display the interactive UI\n",
    "ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e329790a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039a851fb13a4440bd3df84b3e5a018c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Suite:', options=('Data Integrity', 'Train-Test Validation', 'Model Evalu…"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a98abbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: Checkbox(value=True, description='Is Single Value'), 1: Checkbox(value=True, description='Special Characters'), 2: Checkbox(value=True, description='Mixed Nulls'), 3: Checkbox(value=True, description='Mixed Data Types'), 4: Checkbox(value=True, description='String Mismatch'), 5: Checkbox(value=True, description='Data Duplicates')}\n",
      "{6: Checkbox(value=True, description='String Length Out Of Bounds'), 7: Checkbox(value=True, description='Conflicting Labels'), 8: Checkbox(value=True, description='Outlier Sample Detection'), 9: Checkbox(value=True, description='Feature Label Correlation'), 10: Checkbox(value=True, description='Feature Feature Correlation'), 11: Checkbox(value=True, description='Identifier Label Correlation')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f0e33ae14e4de2bc8e4c6957322586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Checkbox(value=True, description='Is Single Value'), Checkbox(value=True, descri…"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import interactive, VBox, HBox, Dropdown, FloatSlider, Button, Checkbox, Output\n",
    "\n",
    "available_suites = {\n",
    "    'Data Integrity': data_integrity(),\n",
    "    'Train-Test Validation': train_test_validation(),\n",
    "    'Model Evaluation': model_evaluation()\n",
    "}\n",
    "\n",
    "\n",
    "# Checkboxes to select which tests to run within the suite\n",
    "test_checkboxes = {test[0]: Checkbox(value=True, description=test[1].name()) \n",
    "                   for test in available_suites['Data Integrity'].checks.items()}\n",
    "\n",
    "left_checkboxes = dict(list(test_checkboxes.items())[:6])\n",
    "print(left_checkboxes)\n",
    "right_checkboxes = dict(list(test_checkboxes.items())[6:])\n",
    "print(right_checkboxes)\n",
    "left_selection_box = VBox(list(left_checkboxes.values()))\n",
    "right_selection_box = VBox(list(right_checkboxes.values()))\n",
    "\n",
    "\n",
    "HBox([left_selection_box, right_selection_box])\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d347c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Checkbox(value=True, description='Is Single Value'),\n",
       " 1: Checkbox(value=True, description='Special Characters'),\n",
       " 2: Checkbox(value=True, description='Mixed Nulls'),\n",
       " 3: Checkbox(value=True, description='Mixed Data Types'),\n",
       " 4: Checkbox(value=True, description='String Mismatch'),\n",
       " 5: Checkbox(value=True, description='Data Duplicates')}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_checkboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2aa96c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsSingleValue\n",
       "\tConditions:\n",
       "\t\t0: Does not contain only a single value"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_suites['Data Integrity'].checks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e55bd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deepchecks.tabular.checks.data_integrity.is_single_value.IsSingleValue"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(available_suites['Data Integrity'].checks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "280227b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0,\n",
       "              IsSingleValue\n",
       "              \tConditions:\n",
       "              \t\t0: Does not contain only a single value),\n",
       "             (1,\n",
       "              SpecialCharacters\n",
       "              \tConditions:\n",
       "              \t\t0: Ratio of samples containing solely special character is less or equal to 0.1%),\n",
       "             (2,\n",
       "              MixedNulls\n",
       "              \tConditions:\n",
       "              \t\t0: Number of different null types is less or equal to 1),\n",
       "             (3,\n",
       "              MixedDataTypes\n",
       "              \tConditions:\n",
       "              \t\t0: Rare data types in column are either more than 10% or less than 1% of the data),\n",
       "             (4,\n",
       "              StringMismatch\n",
       "              \tConditions:\n",
       "              \t\t0: No string variants),\n",
       "             (5,\n",
       "              DataDuplicates\n",
       "              \tConditions:\n",
       "              \t\t0: Duplicate data ratio is less or equal to 5%),\n",
       "             (6,\n",
       "              StringLengthOutOfBounds\n",
       "              \tConditions:\n",
       "              \t\t0: Ratio of string length outliers is less or equal to 0%),\n",
       "             (7,\n",
       "              ConflictingLabels\n",
       "              \tConditions:\n",
       "              \t\t0: Ambiguous sample ratio is less or equal to 0%),\n",
       "             (8, OutlierSampleDetection),\n",
       "             (9,\n",
       "              FeatureLabelCorrelation(ppscore_params={}, random_state=42)\n",
       "              \tConditions:\n",
       "              \t\t0: Features' Predictive Power Score is less than 0.8),\n",
       "             (10,\n",
       "              FeatureFeatureCorrelation\n",
       "              \tConditions:\n",
       "              \t\t0: Not more than 0 pairs are correlated above 0.9),\n",
       "             (11,\n",
       "              IdentifierLabelCorrelation(ppscore_params={})\n",
       "              \tConditions:\n",
       "              \t\t0: Identifier columns PPS is less or equal to 0)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_suites['Data Integrity'].checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "530b0123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_checkboxes = {test: test for test in available_suites['Data Integrity'].checks}\n",
    "test_checkboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "995dc731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(1, 1)\n",
      "(2, 2)\n",
      "(3, 3)\n",
      "(4, 4)\n",
      "(5, 5)\n",
      "(6, 6)\n",
      "(7, 7)\n",
      "(8, 8)\n",
      "(9, 9)\n",
      "(10, 10)\n",
      "(11, 11)\n"
     ]
    }
   ],
   "source": [
    "for test in enumerate(available_suites['Data Integrity'].checks):\n",
    "    print(test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89130cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b9be9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deepchecks\\__init__.py:136: DeprecationWarning:\n",
      "\n",
      "Ability to import base tabular functionality from the `deepchecks` package directly is deprecated, please import from `deepchecks.tabular` instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1245bda14b994c79b4659aaf2360f37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Select Suite:', options=('Data Integrity', 'Train-Test Validation', 'Mode…"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox, Dropdown, Checkbox, Button, Output, IntSlider\n",
    "from docx import Document\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from deepchecks import Suite  # Assuming you are using Deepchecks\n",
    "\n",
    "# Initialize global variables\n",
    "test_checkboxes = {}\n",
    "\n",
    "# Dropdown for selecting suite\n",
    "suite_dropdown = widgets.Dropdown(\n",
    "    options=['Data Integrity', 'Train-Test Validation', 'Model Evaluation'],\n",
    "    value='Data Integrity',\n",
    "    description='Select Suite:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Available suites\n",
    "available_suites = {\n",
    "    'Data Integrity': Suite('Data Integrity Suite'),\n",
    "    'Train-Test Validation': Suite('Train-Test Validation Suite'),\n",
    "    'Model Evaluation': Suite('Model Evaluation Suite')\n",
    "}\n",
    "\n",
    "# Create UI components\n",
    "test_selection_box = widgets.VBox()\n",
    "threshold_slider = widgets.IntSlider(value=50, min=0, max=100, step=1, description='Threshold:')\n",
    "run_button = widgets.Button(description='Run Suite', button_style='success', icon='check')\n",
    "output = widgets.Output()\n",
    "\n",
    "# Update test selection\n",
    "def update_test_selection(change):\n",
    "    global test_checkboxes\n",
    "    selected_suite = change['new']\n",
    "    test_checkboxes = {test[0]: Checkbox(value=True, description=test[1].name()) \n",
    "                       for test in available_suites[selected_suite].checks.items()}\n",
    "    test_selection_box.children = list(test_checkboxes.values())\n",
    "\n",
    "suite_dropdown.observe(update_test_selection, names='value')\n",
    "\n",
    "def run_selected_suite(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(\"Running the suite...\")\n",
    "\n",
    "        # Example logic to run suite (simplified)\n",
    "        output.append_stdout(\"Results saved to Word file.\\n\")\n",
    "\n",
    "run_button.on_click(run_selected_suite)\n",
    "\n",
    "# Display the interactive UI\n",
    "ui = VBox([suite_dropdown, test_selection_box, threshold_slider, run_button, output])\n",
    "ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96a71c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fced340e90547f2a7821b7bb26bf208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Select Suite:', options=('Data Integrity', 'Train-Test Validation', 'Mode…"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox, Dropdown, Checkbox, Button, Output, IntSlider\n",
    "\n",
    "# Step 2: Create UI components\n",
    "suite_dropdown = widgets.Dropdown(\n",
    "    options=['Data Integrity', 'Train-Test Validation', 'Model Evaluation'],\n",
    "    value='Data Integrity',\n",
    "    description='Select Suite:'\n",
    ")\n",
    "\n",
    "test_selection_box = widgets.VBox([\n",
    "    widgets.Checkbox(value=True, description='Test 1'),\n",
    "    widgets.Checkbox(value=False, description='Test 2')\n",
    "])\n",
    "\n",
    "threshold_slider = widgets.IntSlider(\n",
    "    value=50, \n",
    "    min=0, \n",
    "    max=100, \n",
    "    step=1, \n",
    "    description='Threshold:'\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='Run Suite', \n",
    "    button_style='success', \n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# Step 3: Display UI\n",
    "ui = VBox([suite_dropdown, test_selection_box, threshold_slider, run_button, output])\n",
    "ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b70615a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0433b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b181eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox, Dropdown, Checkbox, Button, Output, IntSlider\n",
    "\n",
    "# Step 2: Create UI components\n",
    "suite_dropdown = widgets.Dropdown(\n",
    "    options=['Data Integrity', 'Train-Test Validation', 'Model Evaluation'],\n",
    "    value='Data Integrity',\n",
    "    description='Select Suite:'\n",
    ")\n",
    "\n",
    "# Dynamically change test checkboxes based on the selected suite\n",
    "test_selection_box = widgets.VBox([\n",
    "    widgets.Checkbox(value=True, description='Test 1'),\n",
    "    widgets.Checkbox(value=False, description='Test 2')\n",
    "])\n",
    "\n",
    "# Slider to set threshold\n",
    "threshold_slider = widgets.IntSlider(\n",
    "    value=50, \n",
    "    min=0, \n",
    "    max=100, \n",
    "    step=1, \n",
    "    description='Threshold:'\n",
    ")\n",
    "\n",
    "# Button to run the suite\n",
    "run_button = widgets.Button(\n",
    "    description='Run Suite', \n",
    "    button_style='success', \n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Output to display results\n",
    "output = widgets.Output()\n",
    "\n",
    "# Update UI when the suite selection changes\n",
    "def update_test_selection(change):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(f\"Suite changed to: {change['new']}\")\n",
    "\n",
    "# Attach observer to dropdown\n",
    "suite_dropdown.observe(update_test_selection, names='value')\n",
    "\n",
    "# Run suite logic\n",
    "def run_selected_suite(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        selected_suite = suite_dropdown.value\n",
    "        threshold_value = threshold_slider.value\n",
    "        output.append_stdout(f\"Running suite: {selected_suite} with threshold: {threshold_value}\\n\")\n",
    "        output.append_stdout(f\"Tests selected: {[checkbox.description for checkbox in test_selection_box.children if checkbox.value]}\\n\")\n",
    "\n",
    "# Attach click event to run button\n",
    "run_button.on_click(run_selected_suite)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9178125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6932467882b647e0af1d27ccb8fe9451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Select Suite:', options=('Data Integrity', 'Train-Test Validation', 'Mode…"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Display UI\n",
    "ui = VBox([suite_dropdown, test_selection_box, threshold_slider, run_button, output])\n",
    "ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76156106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbcc52e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961e21c50abb48b9ad0337d7e320d630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "widgets.IntSlider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b4ad5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49db7eecf8ed40edae35e697c3f0a1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Select Suite:', options=('Data Integrity', 'Train-Test Validation', 'Mode…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display  # Added display to ensure UI renders\n",
    "from ipywidgets import VBox, Dropdown, Checkbox, Button, Output, IntSlider\n",
    "\n",
    "# Step 2: Create UI components\n",
    "suite_dropdown = widgets.Dropdown(\n",
    "    options=['Data Integrity', 'Train-Test Validation', 'Model Evaluation'],\n",
    "    value='Data Integrity',\n",
    "    description='Select Suite:'\n",
    ")\n",
    "\n",
    "# Test checkboxes\n",
    "test_selection_box = widgets.VBox([\n",
    "    widgets.Checkbox(value=True, description='Test 1'),\n",
    "    widgets.Checkbox(value=False, description='Test 2')\n",
    "])\n",
    "\n",
    "# Slider to set threshold\n",
    "threshold_slider = widgets.IntSlider(\n",
    "    value=50, \n",
    "    min=0, \n",
    "    max=100, \n",
    "    step=1, \n",
    "    description='Threshold:'\n",
    ")\n",
    "\n",
    "# Button to run the suite\n",
    "run_button = widgets.Button(\n",
    "    description='Run Suite', \n",
    "    button_style='success', \n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Output to display results\n",
    "output = widgets.Output()\n",
    "\n",
    "# Step 3: Display UI\n",
    "ui = VBox([suite_dropdown, test_selection_box, threshold_slider, run_button, output])\n",
    "\n",
    "# **Force display of the UI (important)**\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63843ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70          0.0             1.9      0.076   \n",
       "1            7.8              0.88          0.0             2.6      0.098   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepchecks.tabular.datasets.regression import wine_quality\n",
    "\n",
    "data = wine_quality.load_data(data_format='Dataframe', as_train_test=False)\n",
    "data.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "606a7160-95b3-4e90-a7f7-7ad9e2e56d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data['quality'], test_size=0.2, random_state=42)\n",
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4db3255d-75c1-4a2c-9175-9f8018266963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular import Dataset\n",
    "\n",
    "# Categorical features can be heuristically inferred, however we\n",
    "# recommend to state them explicitly to avoid misclassification.\n",
    "\n",
    "# Metadata attributes are optional. Some checks will run only if specific attributes are declared.\n",
    "\n",
    "train_ds = Dataset(X_train, label=y_train, cat_features=[])\n",
    "test_ds = Dataset(X_test, label=y_test, cat_features=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc02f5fd-95fe-4ec7-8735-f4fe37d46d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a92ead9cb2f4d3abf9ff9ee8683c426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_DHRVSOVJJFL37SJAFIWUV0GLO\">Model Evaluation S…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.tabular.suites import model_evaluation\n",
    "\n",
    "evaluation_suite = model_evaluation()\n",
    "suite_result = evaluation_suite.run(train_ds, test_ds, gbr)\n",
    "# Note: the result can be saved as html using suite_result.save_as_html()\n",
    "# or exported to json using suite_result.to_json()\n",
    "suite_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7be46e-937f-4c8d-850d-f7a9e697a358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aacef9-d375-4c9e-83dc-ae1e8a4310c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408b3aa-8d00-42c0-9a4c-9f6507cfe1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2f4d054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available checks in the Data Integrity Suite:\n",
      "Check 0: Key = 0, Type = IsSingleValue\n",
      "Check 1: Key = 1, Type = SpecialCharacters\n",
      "Check 2: Key = 2, Type = MixedNulls\n",
      "Check 3: Key = 3, Type = MixedDataTypes\n",
      "Check 4: Key = 4, Type = StringMismatch\n",
      "Check 5: Key = 5, Type = DataDuplicates\n",
      "Check 6: Key = 6, Type = StringLengthOutOfBounds\n",
      "Check 7: Key = 7, Type = ConflictingLabels\n",
      "Check 8: Key = 8, Type = OutlierSampleDetection\n",
      "Check 9: Key = 9, Type = FeatureLabelCorrelation\n",
      "Check 10: Key = 10, Type = FeatureFeatureCorrelation\n",
      "Check 11: Key = 11, Type = IdentifierLabelCorrelation\n"
     ]
    }
   ],
   "source": [
    "from deepchecks.tabular.suites import data_integrity\n",
    "from deepchecks.checks import MixedNulls  # Import the correct class for type checking\n",
    "\n",
    "# Step 1: Create the Data Integrity Suite\n",
    "suite = data_integrity()\n",
    "\n",
    "# Step 2: Print the list of checks to verify keys, names, and types\n",
    "print(\"Available checks in the Data Integrity Suite:\")\n",
    "for i, (name, check) in enumerate(suite.checks.items()):\n",
    "    print(f\"Check {i}: Key = {name}, Type = {type(check).__name__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c81ac1bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_condition() missing 1 required positional argument: 'condition_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m mixed_nulls_check\u001b[38;5;241m.\u001b[39mconditions \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# Clears all existing conditions\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 5: Add a new condition with a customized threshold\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmixed_nulls_check\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_condition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproportion of mixed nulls is less than\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Verify the updated conditions\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUpdated conditions for MixedNulls Check:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: add_condition() missing 1 required positional argument: 'condition_func'"
     ]
    }
   ],
   "source": [
    "# Step 3: Access the MixedNulls check using its numeric key (2)\n",
    "if 2 in suite.checks:  # Check that key 2 exists\n",
    "    mixed_nulls_check = suite.checks[2]\n",
    "else:\n",
    "    raise ValueError(\"MixedNulls check not found in the Data Integrity suite.\")\n",
    "\n",
    "# Step 4: Clear the existing conditions (if any)\n",
    "mixed_nulls_check.conditions = {}  # Clears all existing conditions\n",
    "\n",
    "# Step 5: Add a new condition with a customized threshold\n",
    "mixed_nulls_check.add_condition('proportion of mixed nulls is less than', threshold=0.1)\n",
    "\n",
    "# Verify the updated conditions\n",
    "print(\"\\nUpdated conditions for MixedNulls Check:\")\n",
    "print(mixed_nulls_check.conditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ec776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.suites import data_integrity\n",
    "from deepchecks.checks import MixedNulls  # Import the correct class for type checking\n",
    "\n",
    "# Step 1: Create the Data Integrity Suite\n",
    "suite = data_integrity()\n",
    "\n",
    "# Step 2: Print the list of checks to verify keys, names, and types\n",
    "print(\"Available checks in the Data Integrity Suite:\")\n",
    "for i, (name, check) in enumerate(suite.checks.items()):\n",
    "    print(f\"Check {i}: Key = {name}, Type = {type(check).__name__}\")\n",
    "\n",
    "# Step 3: Access the MixedNulls check using its numeric key (2)\n",
    "if 2 in suite.checks:  # Check that key 2 exists\n",
    "    mixed_nulls_check = suite.checks[2]\n",
    "else:\n",
    "    raise ValueError(\"MixedNulls check not found in the Data Integrity suite.\")\n",
    "\n",
    "# Step 4: Clear the existing conditions (if any)\n",
    "mixed_nulls_check.conditions = {}  # Clears all existing conditions\n",
    "\n",
    "# Step 5: Add a new condition with a customized threshold\n",
    "# The condition checks if the result's value (proportion of mixed nulls) is less than 0.1\n",
    "mixed_nulls_check.add_condition(\n",
    "    'proportion of mixed nulls is less than 0.1', \n",
    "    lambda result: result.value < 0.1\n",
    ")\n",
    "\n",
    "# Verify the updated conditions\n",
    "print(\"\\nUpdated conditions for MixedNulls Check:\")\n",
    "print(mixed_nulls_check.conditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea8053-6d25-42a7-88b7-0225a71fe778",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_nulls_check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
