{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b55d6-534f-400b-9382-1ec6dca15931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7259f5cf-57a1-4ea9-9fe7-befe0d78e375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930dc1f550af4d5d998116a026590423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Precision Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.core import CheckResult, ConditionCategory, ConditionResult\n",
    "from deepchecks.tabular import TrainTestCheck, Dataset, Context\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "\n",
    "class PrecisionCheck(TrainTestCheck):\n",
    "    \"\"\"Custom Deepchecks check for evaluating model precision.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def run_logic(self, context: Context) -> CheckResult:\n",
    "        \"\"\"Computes precision score using model predictions on test data.\"\"\"\n",
    "        model = context.model\n",
    "        test_dataset = context.test  # Get test dataset\n",
    "\n",
    "        if model is None or test_dataset is None:\n",
    "            return CheckResult(False, display=\"Model and test dataset are required.\")\n",
    "\n",
    "        if test_dataset.label_name is None:\n",
    "            return CheckResult(False, display=\"Test dataset must have labels.\")\n",
    "\n",
    "        # Extract true labels and predictions\n",
    "        y_true = test_dataset.data[test_dataset.label_name].values\n",
    "        y_pred = model.predict(test_dataset.data[test_dataset.features])\n",
    "\n",
    "        # Compute precision\n",
    "        precision = precision_score(y_true, y_pred, average='binary')\n",
    "\n",
    "        # Return CheckResult with the computed precision score\n",
    "        return CheckResult(value=precision, display=f'Precision: {precision:.4f}')\n",
    "\n",
    "    def add_condition_precision_above(self, threshold: float = 0.80):\n",
    "        \"\"\"Adds a condition to ensure precision is above 0.80.\"\"\"\n",
    "\n",
    "        def condition(result_value):\n",
    "            \"\"\"Condition function to check if precision is above the threshold.\"\"\"\n",
    "            if result_value >= threshold:\n",
    "                return ConditionResult(ConditionCategory.PASS, f\"Precision {result_value:.4f} meets/exceeds {threshold}\")\n",
    "            else:\n",
    "                return ConditionResult(ConditionCategory.FAIL, f\"Precision {result_value:.4f} is below {threshold}\")\n",
    "\n",
    "        # Add the condition to the check instance\n",
    "        return self.add_condition(f'Precision ≥ {threshold}', condition)\n",
    "\n",
    "\n",
    "# Import dependencies for testing the check\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Convert train and test sets to Deepchecks Dataset\n",
    "train_dataset = Dataset(X_train, label=y_train)\n",
    "test_dataset = Dataset(X_test, label=y_test)\n",
    "\n",
    "# Create the precision check instance\n",
    "precision_check = PrecisionCheck().add_condition_precision_above(0.99)\n",
    "\n",
    "# Run the check using train and test datasets\n",
    "result = precision_check.run(train_dataset, test_dataset, model=model)\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfad128f-8fb3-4943-9eb9-a233ab8dc0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72fbc24d1014032acb3a81c52904d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Precision Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.core import CheckResult, ConditionCategory, ConditionResult\n",
    "from deepchecks.tabular import TrainTestCheck, Dataset\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "\n",
    "class PrecisionCheck(TrainTestCheck):\n",
    "    \"\"\"Custom Deepchecks check for evaluating model precision on both train and test datasets.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def run_logic(self, context) -> CheckResult:\n",
    "        \"\"\"Computes precision score for both train and test datasets.\"\"\"\n",
    "        model = context.model\n",
    "        train_dataset = context.train  # Get train dataset\n",
    "        test_dataset = context.test  # Get test dataset\n",
    "\n",
    "        if model is None or train_dataset is None or test_dataset is None:\n",
    "            return CheckResult(False, display=\"Model, train dataset, and test dataset are required.\")\n",
    "\n",
    "        if train_dataset.label_name is None or test_dataset.label_name is None:\n",
    "            return CheckResult(False, display=\"Both train and test datasets must have labels.\")\n",
    "\n",
    "        # Compute precision for train dataset\n",
    "        y_train_true = train_dataset.data[train_dataset.label_name].values\n",
    "        y_train_pred = model.predict(train_dataset.data[train_dataset.features])\n",
    "        train_precision = precision_score(y_train_true, y_train_pred, average='binary')\n",
    "\n",
    "        # Compute precision for test dataset\n",
    "        y_test_true = test_dataset.data[test_dataset.label_name].values\n",
    "        y_test_pred = model.predict(test_dataset.data[test_dataset.features])\n",
    "        test_precision = precision_score(y_test_true, y_test_pred, average='binary')\n",
    "\n",
    "        # Display results\n",
    "        display_str = (f'Train Precision: {train_precision:.4f} \\n'\n",
    "                       f'Test Precision: {test_precision:.4f}')\n",
    "\n",
    "        return CheckResult(value={\"train_precision\": train_precision, \"test_precision\": test_precision}, display=display_str)\n",
    "\n",
    "    def add_condition_precision_above(self, threshold: float = 0.80):\n",
    "        \"\"\"Adds a condition to ensure precision is above a given threshold for both train and test datasets.\"\"\"\n",
    "\n",
    "        def condition(result_value):\n",
    "            \"\"\"Condition function to check if precision is above the threshold.\"\"\"\n",
    "            train_precision = result_value[\"train_precision\"]\n",
    "            test_precision = result_value[\"test_precision\"]\n",
    "\n",
    "            train_status = \"PASS\" if train_precision >= threshold else \"FAIL\"\n",
    "            test_status = \"PASS\" if test_precision >= threshold else \"FAIL\"\n",
    "\n",
    "            message = (f\"Train Precision: {train_precision:.4f} ({train_status})\\n\"\n",
    "                       f\"Test Precision: {test_precision:.4f} ({test_status})\")\n",
    "\n",
    "            if train_precision >= threshold and test_precision >= threshold:\n",
    "                return ConditionResult(ConditionCategory.PASS, message)\n",
    "            else:\n",
    "                return ConditionResult(ConditionCategory.FAIL, message)\n",
    "\n",
    "        return self.add_condition(f'Precision ≥ {threshold} for both train & test', condition)\n",
    "\n",
    "\n",
    "# Import dependencies for testing the check\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Convert train and test sets to Deepchecks Dataset\n",
    "train_dataset = Dataset(X_train, label=y_train)\n",
    "test_dataset = Dataset(X_test, label=y_test)\n",
    "\n",
    "# Create the precision check instance\n",
    "precision_check = PrecisionCheck().add_condition_precision_above(0.80)\n",
    "\n",
    "# Run the check using train and test datasets\n",
    "result = precision_check.run(train_dataset, test_dataset, model=model)\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "911236cc-6941-44de-a53a-08714111c20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0dffb79d674976be0a67847a591fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Precision Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783c1cc143d44e0c9fc805d05e606ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Precision Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.core import CheckResult, ConditionCategory, ConditionResult\n",
    "from deepchecks.tabular import TrainTestCheck, Dataset\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "class PrecisionCheck(TrainTestCheck):\n",
    "    \"\"\"Custom Deepchecks check for evaluating precision with or without a model.\"\"\"\n",
    "\n",
    "    def __init__(self, use_model=True, y_train_pred=None, y_test_pred=None, **kwargs):\n",
    "        \"\"\"\n",
    "        use_model: If True, use the model for prediction. If False, rely on provided predicted labels.\n",
    "        y_train_pred: Predicted labels for train set (if use_model=False).\n",
    "        y_test_pred: Predicted labels for test set (if use_model=False).\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.use_model = use_model  # Control whether to use the model\n",
    "        self.y_train_pred = y_train_pred\n",
    "        self.y_test_pred = y_test_pred\n",
    "\n",
    "    def run_logic(self, context) -> CheckResult:\n",
    "        \"\"\"Computes precision score for both train and test datasets.\"\"\"\n",
    "        train_dataset = context.train\n",
    "        test_dataset = context.test\n",
    "        model = context.model if self.use_model else None  # Use model only if enabled\n",
    "\n",
    "        if train_dataset is None or test_dataset is None:\n",
    "            return CheckResult(False, display=\"Train and test datasets are required.\")\n",
    "\n",
    "        if train_dataset.label_name is None or test_dataset.label_name is None:\n",
    "            return CheckResult(False, display=\"Both train and test datasets must have actual labels.\")\n",
    "\n",
    "        # Extract true labels\n",
    "        y_train_true = train_dataset.data[train_dataset.label_name].values\n",
    "        y_test_true = test_dataset.data[test_dataset.label_name].values\n",
    "\n",
    "        # Get predictions: Either from the model or from provided predicted labels\n",
    "        if self.use_model:\n",
    "            if model is None:\n",
    "                return CheckResult(False, display=\"Model is required but not provided.\")\n",
    "            y_train_pred = model.predict(train_dataset.data[train_dataset.features])\n",
    "            y_test_pred = model.predict(test_dataset.data[test_dataset.features])\n",
    "        else:\n",
    "            if self.y_train_pred is None or self.y_test_pred is None:\n",
    "                return CheckResult(False, display=\"Predicted labels for train and test must be provided when use_model=False.\")\n",
    "            y_train_pred = self.y_train_pred\n",
    "            y_test_pred = self.y_test_pred\n",
    "\n",
    "        # Compute precision\n",
    "        train_precision = precision_score(y_train_true, y_train_pred, average='binary')\n",
    "        test_precision = precision_score(y_test_true, y_test_pred, average='binary')\n",
    "\n",
    "        display_str = (f'Train Precision: {train_precision:.4f} \\n'\n",
    "                       f'Test Precision: {test_precision:.4f}')\n",
    "\n",
    "        return CheckResult(value={\"train_precision\": train_precision, \"test_precision\": test_precision}, display=display_str)\n",
    "\n",
    "    def add_condition_precision_above(self, threshold: float = 0.80):\n",
    "        \"\"\"Adds a condition to ensure precision is above a given threshold.\"\"\"\n",
    "\n",
    "        def condition(result_value):\n",
    "            train_precision = result_value[\"train_precision\"]\n",
    "            test_precision = result_value[\"test_precision\"]\n",
    "\n",
    "            train_status = \"PASS\" if train_precision >= threshold else \"FAIL\"\n",
    "            test_status = \"PASS\" if test_precision >= threshold else \"FAIL\"\n",
    "\n",
    "            message = (f\"Train Precision: {train_precision:.4f} ({train_status})\\n\"\n",
    "                       f\"Test Precision: {test_precision:.4f} ({test_status})\")\n",
    "\n",
    "            if train_precision >= threshold and test_precision >= threshold:\n",
    "                return ConditionResult(ConditionCategory.PASS, message)\n",
    "            else:\n",
    "                return ConditionResult(ConditionCategory.FAIL, message)\n",
    "\n",
    "        return self.add_condition(f'Precision ≥ {threshold} for both train & test', condition)\n",
    "\n",
    "\n",
    "# Import dependencies for testing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Simulate predicted labels (e.g., from a pre-saved model output)\n",
    "y_train_pred = np.random.choice([0, 1], size=len(y_train))  # Simulated train predictions\n",
    "y_test_pred = np.random.choice([0, 1], size=len(y_test))  # Simulated test predictions\n",
    "\n",
    "# Convert train and test sets to Deepchecks Dataset\n",
    "train_dataset = Dataset(X_train, label=y_train)\n",
    "test_dataset = Dataset(X_test, label=y_test)\n",
    "\n",
    "# Option 1: Run the check using a model\n",
    "precision_check_model = PrecisionCheck(use_model=True).add_condition_precision_above(0.80)\n",
    "result_model = precision_check_model.run(train_dataset, test_dataset, model=model)\n",
    "result_model.show()\n",
    "\n",
    "# Option 2: Run the check using only predicted labels (no model)\n",
    "precision_check_no_model = PrecisionCheck(use_model=False, y_train_pred=y_train_pred, y_test_pred=y_test_pred)\n",
    "precision_check_no_model.add_condition_precision_above(0.80)\n",
    "result_no_model = precision_check_no_model.run(train_dataset, test_dataset)\n",
    "result_no_model.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c3177ed-71ad-4942-975b-8e8706364d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f5f31a192e46efa78ff3be56e737d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Precision Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Option 2: Run the check using only predicted labels (no model)\n",
    "precision_check_no_model = PrecisionCheck(use_model=False, y_train_pred=y_train_pred, y_test_pred=y_test_pred)\n",
    "precision_check_no_model.add_condition_precision_above(0.80)\n",
    "result_no_model = precision_check_no_model.run(train_dataset, test_dataset)\n",
    "result_no_model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e37c13-0fe9-422b-bba2-97ef845f3262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606138e166a14e328124d6a8bffcd670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_F3JG4RG9XBWR6AZSDVNK122L3\">Model Evaluation S…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_eval_suite = Suite(\"Model Evaluation Suite\")\n",
    "model_eval_suite.add(precision_check)\n",
    "\n",
    "# Run the suite\n",
    "suite_result = model_eval_suite.run(train_dataset, test_dataset, model=model)\n",
    "\n",
    "# Show results\n",
    "suite_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550b09f-e2bd-40a7-941a-f81811ac6b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
