{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84b55d6-534f-400b-9382-1ec6dca15931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - You are using deepchecks version 0.19.0, however a newer version is available. Deepchecks is frequently updated with major improvements. You should consider upgrading via the \"python -m pip install --upgrade deepchecks\" command.\n"
     ]
    }
   ],
   "source": [
    "import deepchecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7259f5cf-57a1-4ea9-9fe7-befe0d78e375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f11c0ddb1214342830c8f91ac5eed44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Precision Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.core import CheckResult, ConditionCategory, ConditionResult\n",
    "from deepchecks.tabular import TrainTestCheck, Dataset, Context\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "\n",
    "class PrecisionCheck(TrainTestCheck):\n",
    "    \"\"\"Custom Deepchecks check for evaluating model precision.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        #self.name = \"F1 Score\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def run_logic(self, context: Context) -> CheckResult:\n",
    "        \"\"\"Computes precision score using model predictions on test data.\"\"\"\n",
    "        model = context.model\n",
    "        test_dataset = context.test  # Get test dataset\n",
    "\n",
    "        if model is None or test_dataset is None:\n",
    "            return CheckResult(False, display=\"Model and test dataset are required.\")\n",
    "\n",
    "        if test_dataset.label_name is None:\n",
    "            return CheckResult(False, display=\"Test dataset must have labels.\")\n",
    "\n",
    "        # Extract true labels and predictions\n",
    "        y_true = test_dataset.data[test_dataset.label_name].values\n",
    "        y_pred = model.predict(test_dataset.data[test_dataset.features])\n",
    "\n",
    "        # Compute precision\n",
    "        precision = precision_score(y_true, y_pred, average='binary')\n",
    "\n",
    "        # Return CheckResult with the computed precision score\n",
    "        return CheckResult(value=precision, display=f'Precision: {precision:.4f}')\n",
    "\n",
    "    def add_condition_precision_above(self, threshold: float = 0.80):\n",
    "        \"\"\"Adds a condition to ensure precision is above 0.80.\"\"\"\n",
    "\n",
    "        def condition(result_value):\n",
    "            \"\"\"Condition function to check if precision is above the threshold.\"\"\"\n",
    "            if result_value >= threshold:\n",
    "                return ConditionResult(ConditionCategory.PASS, f\"Precision {result_value:.4f} meets/exceeds {threshold}\")\n",
    "            else:\n",
    "                return ConditionResult(ConditionCategory.FAIL, f\"Precision {result_value:.4f} is below {threshold}\")\n",
    "\n",
    "        # Add the condition to the check instance\n",
    "        return self.add_condition(f'Precision ≥ {threshold}', condition)\n",
    "\n",
    "\n",
    "# Import dependencies for testing the check\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Convert train and test sets to Deepchecks Dataset\n",
    "train_dataset = Dataset(X_train, label=y_train)\n",
    "test_dataset = Dataset(X_test, label=y_test)\n",
    "\n",
    "# Create the precision check instance\n",
    "precision_check = PrecisionCheck().add_condition_precision_above(0.99)\n",
    "\n",
    "# Run the check using train and test datasets\n",
    "result = precision_check.run(train_dataset, test_dataset, model=model)\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daa135ab-d8c5-424a-aa0d-b3a4ee5ccf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6d4a36f83f494daa3e1331d51cf396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>F1 Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evaluating m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.core import CheckResult, ConditionCategory, ConditionResult\n",
    "from deepchecks.tabular import TrainTestCheck, Dataset, Context\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "class F1Check(TrainTestCheck):\n",
    "    \"\"\"Custom Deepchecks check for evaluating model F1.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        #self.name = \"F1 Score\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def run_logic(self, context: Context) -> CheckResult:\n",
    "        \"\"\"Computes F1 score using model predictions on test data.\"\"\"\n",
    "        model = context.model\n",
    "        test_dataset = context.test  # Get test dataset\n",
    "\n",
    "        if model is None or test_dataset is None:\n",
    "            return CheckResult(False, display=\"Model and test dataset are required.\")\n",
    "\n",
    "        if test_dataset.label_name is None:\n",
    "            return CheckResult(False, display=\"Test dataset must have labels.\")\n",
    "\n",
    "        # Extract true labels and predictions\n",
    "        y_true = test_dataset.data[test_dataset.label_name].values\n",
    "        y_pred = model.predict(test_dataset.data[test_dataset.features])\n",
    "\n",
    "        # Compute F1\n",
    "        F1 = f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "        # Return CheckResult with the computed F1 score\n",
    "        return CheckResult(value=F1, display=f'F1: {F1:.4f}')\n",
    "\n",
    "    def add_condition_F1_above(self, threshold: float = 0.80):\n",
    "        \"\"\"Adds a condition to ensure F1 is above 0.80.\"\"\"\n",
    "\n",
    "        def condition(result_value):\n",
    "            \"\"\"Condition function to check if F1 is above the threshold.\"\"\"\n",
    "            if result_value >= threshold:\n",
    "                return ConditionResult(ConditionCategory.PASS, f\"F1 {result_value:.4f} meets/exceeds {threshold}\")\n",
    "            else:\n",
    "                return ConditionResult(ConditionCategory.FAIL, f\"F1 {result_value:.4f} is below {threshold}\")\n",
    "\n",
    "        # Add the condition to the check instance\n",
    "        return self.add_condition(f'F1 ≥ {threshold}', condition)\n",
    "\n",
    "\n",
    "# Import dependencies for testing the check\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Convert train and test sets to Deepchecks Dataset\n",
    "train_dataset = Dataset(X_train, label=y_train)\n",
    "test_dataset = Dataset(X_test, label=y_test)\n",
    "\n",
    "# Create the F1 check instance\n",
    "F1_check = F1Check().add_condition_F1_above(0.99)\n",
    "\n",
    "# Run the check using train and test datasets\n",
    "result = F1_check.run(train_dataset, test_dataset, model=model)\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e382ff1c-08c6-4652-a36c-a798b7d745cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6388ee-2bf1-472d-938e-c0ae1774134e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57eac4-9a77-49bb-83ea-f1daec7cf745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfad128f-8fb3-4943-9eb9-a233ab8dc0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faadb4aea60942209bcbeb4d38ba58a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Precision Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.core import CheckResult, ConditionCategory, ConditionResult\n",
    "from deepchecks.tabular import TrainTestCheck, Dataset\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "\n",
    "class PrecisionCheck(TrainTestCheck):\n",
    "    \"\"\"Custom Deepchecks check for evaluating model precision on both train and test datasets.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def run_logic(self, context) -> CheckResult:\n",
    "        \"\"\"Computes precision score for both train and test datasets.\"\"\"\n",
    "        model = context.model\n",
    "        train_dataset = context.train  # Get train dataset\n",
    "        test_dataset = context.test  # Get test dataset\n",
    "\n",
    "        if model is None or train_dataset is None or test_dataset is None:\n",
    "            return CheckResult(False, display=\"Model, train dataset, and test dataset are required.\")\n",
    "\n",
    "        if train_dataset.label_name is None or test_dataset.label_name is None:\n",
    "            return CheckResult(False, display=\"Both train and test datasets must have labels.\")\n",
    "\n",
    "        # Compute precision for train dataset\n",
    "        y_train_true = train_dataset.data[train_dataset.label_name].values\n",
    "        y_train_pred = model.predict(train_dataset.data[train_dataset.features])\n",
    "        train_precision = precision_score(y_train_true, y_train_pred, average='binary')\n",
    "\n",
    "        # Compute precision for test dataset\n",
    "        y_test_true = test_dataset.data[test_dataset.label_name].values\n",
    "        y_test_pred = model.predict(test_dataset.data[test_dataset.features])\n",
    "        test_precision = precision_score(y_test_true, y_test_pred, average='binary')\n",
    "\n",
    "        # Display results\n",
    "        display_str = (f'Train Precision: {train_precision:.4f} \\n'\n",
    "                       f'Test Precision: {test_precision:.4f}')\n",
    "\n",
    "        return CheckResult(value={\"train_precision\": train_precision, \"test_precision\": test_precision}, display=display_str)\n",
    "\n",
    "    def add_condition_precision_above(self, threshold: float = 0.80):\n",
    "        \"\"\"Adds a condition to ensure precision is above a given threshold for both train and test datasets.\"\"\"\n",
    "\n",
    "        def condition(result_value):\n",
    "            \"\"\"Condition function to check if precision is above the threshold.\"\"\"\n",
    "            train_precision = result_value[\"train_precision\"]\n",
    "            test_precision = result_value[\"test_precision\"]\n",
    "\n",
    "            train_status = \"PASS\" if train_precision >= threshold else \"FAIL\"\n",
    "            test_status = \"PASS\" if test_precision >= threshold else \"FAIL\"\n",
    "\n",
    "            message = (f\"Train Precision: {train_precision:.4f} ({train_status})\\n\"\n",
    "                       f\"Test Precision: {test_precision:.4f} ({test_status})\")\n",
    "\n",
    "            if train_precision >= threshold and test_precision >= threshold:\n",
    "                return ConditionResult(ConditionCategory.PASS, message)\n",
    "            else:\n",
    "                return ConditionResult(ConditionCategory.FAIL, message)\n",
    "\n",
    "        return self.add_condition(f'Precision ≥ {threshold} for both train & test', condition)\n",
    "\n",
    "\n",
    "# Import dependencies for testing the check\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Convert train and test sets to Deepchecks Dataset\n",
    "train_dataset = Dataset(X_train, label=y_train)\n",
    "test_dataset = Dataset(X_test, label=y_test)\n",
    "\n",
    "# Create the precision check instance\n",
    "precision_check = PrecisionCheck().add_condition_precision_above(0.80)\n",
    "\n",
    "# Run the check using train and test datasets\n",
    "result = precision_check.run(train_dataset, test_dataset, model=model)\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92edb562-1028-43d5-b324-43c3e82e61a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53395a667c664d54ba7aeb7ea714b854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>F1 Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evaluating m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.core import CheckResult, ConditionCategory, ConditionResult\n",
    "from deepchecks.tabular import TrainTestCheck, Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "class F1Check(TrainTestCheck):\n",
    "    \"\"\"Custom Deepchecks check for evaluating model F1 on both train and test datasets.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def run_logic(self, context) -> CheckResult:\n",
    "        \"\"\"Computes F1 score for both train and test datasets.\"\"\"\n",
    "        model = context.model\n",
    "        train_dataset = context.train  # Get train dataset\n",
    "        test_dataset = context.test  # Get test dataset\n",
    "\n",
    "        if model is None or train_dataset is None or test_dataset is None:\n",
    "            return CheckResult(False, display=\"Model, train dataset, and test dataset are required.\")\n",
    "\n",
    "        if train_dataset.label_name is None or test_dataset.label_name is None:\n",
    "            return CheckResult(False, display=\"Both train and test datasets must have labels.\")\n",
    "\n",
    "        # Compute F1 for train dataset\n",
    "        y_train_true = train_dataset.data[train_dataset.label_name].values\n",
    "        y_train_pred = model.predict(train_dataset.data[train_dataset.features])\n",
    "        train_F1 = f1_score(y_train_true, y_train_pred, average='binary')\n",
    "\n",
    "        # Compute F1 for test dataset\n",
    "        y_test_true = test_dataset.data[test_dataset.label_name].values\n",
    "        y_test_pred = model.predict(test_dataset.data[test_dataset.features])\n",
    "        test_F1 = f1_score(y_test_true, y_test_pred, average='binary')\n",
    "\n",
    "        # Display results\n",
    "        display_str = (f'Train F1: {train_F1:.4f} \\n'\n",
    "                       f'Test F1: {test_F1:.4f}')\n",
    "\n",
    "        return CheckResult(value={\"train_F1\": train_F1, \"test_F1\": test_F1}, display=display_str)\n",
    "\n",
    "    def add_condition_F1_above(self, threshold: float = 0.80):\n",
    "        \"\"\"Adds a condition to ensure F1 is above a given threshold for both train and test datasets.\"\"\"\n",
    "\n",
    "        def condition(result_value):\n",
    "            \"\"\"Condition function to check if F1 is above the threshold.\"\"\"\n",
    "            train_F1 = result_value[\"train_F1\"]\n",
    "            test_F1 = result_value[\"test_F1\"]\n",
    "\n",
    "            train_status = \"PASS\" if train_F1 >= threshold else \"FAIL\"\n",
    "            test_status = \"PASS\" if test_F1 >= threshold else \"FAIL\"\n",
    "\n",
    "            message = (f\"Train F1: {train_F1:.4f} ({train_status})\\n\"\n",
    "                       f\"Test F1: {test_F1:.4f} ({test_status})\")\n",
    "\n",
    "            if train_F1 >= threshold and test_F1 >= threshold:\n",
    "                return ConditionResult(ConditionCategory.PASS, message)\n",
    "            else:\n",
    "                return ConditionResult(ConditionCategory.FAIL, message)\n",
    "\n",
    "        return self.add_condition(f'F1 ≥ {threshold} for both train & test', condition)\n",
    "\n",
    "\n",
    "# Import dependencies for testing the check\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Convert train and test sets to Deepchecks Dataset\n",
    "train_dataset = Dataset(X_train, label=y_train)\n",
    "test_dataset = Dataset(X_test, label=y_test)\n",
    "\n",
    "# Create the F1 check instance\n",
    "F1_check = F1Check().add_condition_F1_above(0.80)\n",
    "\n",
    "# Run the check using train and test datasets\n",
    "result = F1_check.run(train_dataset, test_dataset, model=model)\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cecb49-fb4b-4b5b-a4ec-ff91ea3cd967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41fc327-152e-4ee5-a9f8-1bc2a9722930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0c69d-24a5-4bfb-814b-4c4bf5c48f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f9e03-f943-4505-87f8-f54a547fbb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d95c7f-3f22-4d08-bef5-5083710b598c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "911236cc-6941-44de-a53a-08714111c20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15451a19e6c48149d65ec320fae495b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Precision Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e7a121173243c4b354a1e53bcd3746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Precision Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.core import CheckResult, ConditionCategory, ConditionResult\n",
    "from deepchecks.tabular import TrainTestCheck, Dataset\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "class PrecisionCheck(TrainTestCheck):\n",
    "    \"\"\"Custom Deepchecks check for evaluating precision with or without a model.\"\"\"\n",
    "\n",
    "    def __init__(self, use_model=True, y_train_pred=None, y_test_pred=None, **kwargs):\n",
    "        \"\"\"\n",
    "        use_model: If True, use the model for prediction. If False, rely on provided predicted labels.\n",
    "        y_train_pred: Predicted labels for train set (if use_model=False).\n",
    "        y_test_pred: Predicted labels for test set (if use_model=False).\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.use_model = use_model  # Control whether to use the model\n",
    "        self.y_train_pred = y_train_pred\n",
    "        self.y_test_pred = y_test_pred\n",
    "\n",
    "    def run_logic(self, context) -> CheckResult:\n",
    "        \"\"\"Computes precision score for both train and test datasets.\"\"\"\n",
    "        train_dataset = context.train\n",
    "        test_dataset = context.test\n",
    "        model = context.model if self.use_model else None  # Use model only if enabled\n",
    "\n",
    "        if train_dataset is None or test_dataset is None:\n",
    "            return CheckResult(False, display=\"Train and test datasets are required.\")\n",
    "\n",
    "        if train_dataset.label_name is None or test_dataset.label_name is None:\n",
    "            return CheckResult(False, display=\"Both train and test datasets must have actual labels.\")\n",
    "\n",
    "        # Extract true labels\n",
    "        y_train_true = train_dataset.data[train_dataset.label_name].values\n",
    "        y_test_true = test_dataset.data[test_dataset.label_name].values\n",
    "\n",
    "        # Get predictions: Either from the model or from provided predicted labels\n",
    "        if self.use_model:\n",
    "            if model is None:\n",
    "                return CheckResult(False, display=\"Model is required but not provided.\")\n",
    "            y_train_pred = model.predict(train_dataset.data[train_dataset.features])\n",
    "            y_test_pred = model.predict(test_dataset.data[test_dataset.features])\n",
    "        else:\n",
    "            if self.y_train_pred is None or self.y_test_pred is None:\n",
    "                return CheckResult(False, display=\"Predicted labels for train and test must be provided when use_model=False.\")\n",
    "            y_train_pred = self.y_train_pred\n",
    "            y_test_pred = self.y_test_pred\n",
    "\n",
    "        # Compute precision\n",
    "        train_precision = precision_score(y_train_true, y_train_pred, average='binary')\n",
    "        test_precision = precision_score(y_test_true, y_test_pred, average='binary')\n",
    "\n",
    "        display_str = (f'Train Precision: {train_precision:.4f} \\n'\n",
    "                       f'Test Precision: {test_precision:.4f}')\n",
    "\n",
    "        return CheckResult(value={\"train_precision\": train_precision, \"test_precision\": test_precision}, display=display_str)\n",
    "\n",
    "    def add_condition_precision_above(self, threshold: float = 0.80):\n",
    "        \"\"\"Adds a condition to ensure precision is above a given threshold.\"\"\"\n",
    "\n",
    "        def condition(result_value):\n",
    "            train_precision = result_value[\"train_precision\"]\n",
    "            test_precision = result_value[\"test_precision\"]\n",
    "\n",
    "            train_status = \"PASS\" if train_precision >= threshold else \"FAIL\"\n",
    "            test_status = \"PASS\" if test_precision >= threshold else \"FAIL\"\n",
    "\n",
    "            message = (f\"Train Precision: {train_precision:.4f} ({train_status})\\n\"\n",
    "                       f\"Test Precision: {test_precision:.4f} ({test_status})\")\n",
    "\n",
    "            if train_precision >= threshold and test_precision >= threshold:\n",
    "                return ConditionResult(ConditionCategory.PASS, message)\n",
    "            else:\n",
    "                return ConditionResult(ConditionCategory.FAIL, message)\n",
    "\n",
    "        return self.add_condition(f'Precision ≥ {threshold} for both train & test', condition)\n",
    "\n",
    "\n",
    "# Import dependencies for testing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Simulate predicted labels (e.g., from a pre-saved model output)\n",
    "y_train_pred = np.random.choice([0, 1], size=len(y_train))  # Simulated train predictions\n",
    "y_test_pred = np.random.choice([0, 1], size=len(y_test))  # Simulated test predictions\n",
    "\n",
    "# Convert train and test sets to Deepchecks Dataset\n",
    "train_dataset = Dataset(X_train, label=y_train)\n",
    "test_dataset = Dataset(X_test, label=y_test)\n",
    "\n",
    "# Option 1: Run the check using a model\n",
    "precision_check_model = PrecisionCheck(use_model=True).add_condition_precision_above(0.80)\n",
    "result_model = precision_check_model.run(train_dataset, test_dataset, model=model)\n",
    "result_model.show()\n",
    "\n",
    "# Option 2: Run the check using only predicted labels (no model)\n",
    "precision_check_no_model = PrecisionCheck(use_model=False, y_train_pred=y_train_pred, y_test_pred=y_test_pred)\n",
    "precision_check_no_model.add_condition_precision_above(0.80)\n",
    "result_no_model = precision_check_no_model.run(train_dataset, test_dataset)\n",
    "result_no_model.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c3177ed-71ad-4942-975b-8e8706364d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482f93a8a0e44f13820dd3ae94c53ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Precision Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evalu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Option 2: Run the check using only predicted labels (no model)\n",
    "precision_check_no_model = PrecisionCheck(use_model=False, y_train_pred=y_train_pred, y_test_pred=y_test_pred)\n",
    "precision_check_no_model.add_condition_precision_above(0.80)\n",
    "result_no_model = precision_check_no_model.run(train_dataset, test_dataset)\n",
    "result_no_model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd57239a-fddc-45e8-b203-27ca8726ed96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfc26b5faa34cac9d857e15cdb7069f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>F1 Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evaluating F…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46505534f9bf4a8a83bb5505d65d2269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>F1 Check</b></h4>'), HTML(value='<p>Custom Deepchecks check for evaluating F…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.core import CheckResult, ConditionCategory, ConditionResult\n",
    "from deepchecks.tabular import TrainTestCheck, Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class F1Check(TrainTestCheck):\n",
    "    \"\"\"Custom Deepchecks check for evaluating F1 with or without a model.\"\"\"\n",
    "\n",
    "    def __init__(self, use_model=True, y_train_pred=None, y_test_pred=None, **kwargs):\n",
    "        \"\"\"\n",
    "        use_model: If True, use the model for prediction. If False, rely on provided predicted labels.\n",
    "        y_train_pred: Predicted labels for train set (if use_model=False).\n",
    "        y_test_pred: Predicted labels for test set (if use_model=False).\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.use_model = use_model  # Control whether to use the model\n",
    "        self.y_train_pred = y_train_pred\n",
    "        self.y_test_pred = y_test_pred\n",
    "\n",
    "    def run_logic(self, context) -> CheckResult:\n",
    "        \"\"\"Computes F1 score for both train and test datasets.\"\"\"\n",
    "        train_dataset = context.train\n",
    "        test_dataset = context.test\n",
    "        model = context.model if self.use_model else None  # Use model only if enabled\n",
    "\n",
    "        if train_dataset is None or test_dataset is None:\n",
    "            return CheckResult(False, display=\"Train and test datasets are required.\")\n",
    "\n",
    "        if train_dataset.label_name is None or test_dataset.label_name is None:\n",
    "            return CheckResult(False, display=\"Both train and test datasets must have actual labels.\")\n",
    "\n",
    "        # Extract true labels\n",
    "        y_train_true = train_dataset.data[train_dataset.label_name].values\n",
    "        y_test_true = test_dataset.data[test_dataset.label_name].values\n",
    "\n",
    "        # Get predictions: Either from the model or from provided predicted labels\n",
    "        if self.use_model:\n",
    "            if model is None:\n",
    "                return CheckResult(False, display=\"Model is required but not provided.\")\n",
    "            y_train_pred = model.predict(train_dataset.data[train_dataset.features])\n",
    "            y_test_pred = model.predict(test_dataset.data[test_dataset.features])\n",
    "        else:\n",
    "            if self.y_train_pred is None or self.y_test_pred is None:\n",
    "                return CheckResult(False, display=\"Predicted labels for train and test must be provided when use_model=False.\")\n",
    "            y_train_pred = self.y_train_pred\n",
    "            y_test_pred = self.y_test_pred\n",
    "\n",
    "        # Compute F1\n",
    "        train_F1 = f1_score(y_train_true, y_train_pred, average='binary')\n",
    "        test_F1 = f1_score(y_test_true, y_test_pred, average='binary')\n",
    "\n",
    "        display_str = (f'Train F1: {train_F1:.4f} \\n'\n",
    "                       f'Test F1: {test_F1:.4f}')\n",
    "\n",
    "        return CheckResult(value={\"train_F1\": train_F1, \"test_F1\": test_F1}, display=display_str)\n",
    "\n",
    "    def add_condition_F1_above(self, threshold: float = 0.80):\n",
    "        \"\"\"Adds a condition to ensure F1 is above a given threshold.\"\"\"\n",
    "\n",
    "        def condition(result_value):\n",
    "            train_F1 = result_value[\"train_F1\"]\n",
    "            test_F1 = result_value[\"test_F1\"]\n",
    "\n",
    "            train_status = \"PASS\" if train_F1 >= threshold else \"FAIL\"\n",
    "            test_status = \"PASS\" if test_F1 >= threshold else \"FAIL\"\n",
    "\n",
    "            message = (f\"Train F1: {train_F1:.4f} ({train_status})\\n\"\n",
    "                       f\"Test F1: {test_F1:.4f} ({test_status})\")\n",
    "\n",
    "            if train_F1 >= threshold and test_F1 >= threshold:\n",
    "                return ConditionResult(ConditionCategory.PASS, message)\n",
    "            else:\n",
    "                return ConditionResult(ConditionCategory.FAIL, message)\n",
    "\n",
    "        return self.add_condition(f'F1 ≥ {threshold} for both train & test', condition)\n",
    "\n",
    "\n",
    "# Import dependencies for testing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Simulate predicted labels (e.g., from a pre-saved model output)\n",
    "y_train_pred = np.random.choice([0, 1], size=len(y_train))  # Simulated train predictions\n",
    "y_test_pred = np.random.choice([0, 1], size=len(y_test))  # Simulated test predictions\n",
    "\n",
    "# Convert train and test sets to Deepchecks Dataset\n",
    "train_dataset = Dataset(X_train, label=y_train)\n",
    "test_dataset = Dataset(X_test, label=y_test)\n",
    "\n",
    "# Option 1: Run the check using a model\n",
    "F1_check_model = F1Check(use_model=True).add_condition_F1_above(0.80)\n",
    "result_model = F1_check_model.run(train_dataset, test_dataset, model=model)\n",
    "result_model.show()\n",
    "\n",
    "# Option 2: Run the check using only predicted labels (no model)\n",
    "F1_check_no_model = F1Check(use_model=False, y_train_pred=y_train_pred, y_test_pred=y_test_pred)\n",
    "F1_check_no_model.add_condition_F1_above(0.80)\n",
    "result_no_model = F1_check_no_model.run(train_dataset, test_dataset)\n",
    "result_no_model.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0218e8-68d1-447e-80aa-dcbecc665f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361ae9e-136d-4e09-9b81-467b032bc8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799f700-26c6-4f2e-af18-c3a8570e7fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cf56c-ea56-436a-8523-ca2a892760a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a796b2-ea17-4ce8-ba8c-9c90309bd008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a7ae8-739e-4438-a75d-1898d6c95aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e37c13-0fe9-422b-bba2-97ef845f3262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606138e166a14e328124d6a8bffcd670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_F3JG4RG9XBWR6AZSDVNK122L3\">Model Evaluation S…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_eval_suite = Suite(\"Model Evaluation Suite\")\n",
    "model_eval_suite.add(precision_check)\n",
    "\n",
    "# Run the suite\n",
    "suite_result = model_eval_suite.run(train_dataset, test_dataset, model=model)\n",
    "\n",
    "# Show results\n",
    "suite_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550b09f-e2bd-40a7-941a-f81811ac6b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
